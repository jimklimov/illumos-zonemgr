#! /usr/bin/env bash

# Portability Note: BASH-specific syntax is likely present below, so
# the interpreter is fixed in the shebang above. YMMV with other shells.
#
# This is a script to manage illumos zones lifecycle for essentially
# disposable zones -- such as single-use build environments that are
# clones of a golden template env, which has its own update schedule
# (if any).
#
# It is maintained at https://github.com/jimklimov/illumos-zonemgr
#
#   LICENSE:
#
# This file and its contents are supplied under the terms of the
# Common Development and Distribution License ("CDDL"). You may
# only use this file in accordance with the terms of the CDDL.
#
# A full copy of the text of the CDDL should have accompanied this
# source. A copy of the CDDL is also available via the Internet at
# http://www.illumos.org/license/CDDL.
#

#
# Copyright 2016-2017, Jim Klimov. All rights reserved.
#

##########################################################################
# General helpers
##########################################################################

if [ -n "${BASH-}" ]; then
    # A bash-ism, should set the exitcode of the rightmost failed command
    # in a pipeline, otherwise e.g. exitcode("false | true") == 0
    set -o pipefail 2>/dev/null || \
        echo "WARNING: Could not set up Bash pipefail mode"
    echo_E() { echo -E "$@"; }
    echo_e() { echo -e "$@"; }
else
    echo_E() { /bin/echo -E "$@"; }
    echo_e() { /bin/echo -e "$@"; }
fi

## Store some important CLI values. DO NOT "export" THESE!
[ -z "${_SCRIPT_STARTPWD-}" ] && _SCRIPT_STARTPWD="`pwd`"
[ -z "${_SCRIPT_PATH-}" ] && _SCRIPT_PATH="$0"
[ -z "${_SCRIPT_NAME-}" ] && _SCRIPT_NAME="`basename "${_SCRIPT_PATH}"`"
_SCRIPT_ARGS="$*"
_SCRIPT_ARGC="$#"

### Set the default language
[ -z "${LANG-}" ] && LANG=C
[ -z "${LANGUAGE-}" ] && LANGUAGE=C
[ -z "${LC_ALL-}" ] && LC_ALL=C
[ -z "${TZ-}" ] && TZ=UTC
export LANG LANGUAGE LC_ALL TZ

PATH="/sbin:/usr/sbin:/usr/bin:/usr/gnu/bin:/usr/sfw/bin:$PATH"
export PATH

### Empty and non-numeric and non-positive values should be filtered
### out here
is_positive() {
    [ -n "$1" -a "$1" -gt 0 ] 2>/dev/null
}
default_posval() {
    eval is_positive "\$$1" || eval "$1"="$2"
}

### Caller can disable specific debuggers by setting their level too
### high in its environment variables for a specific script run.
### Scripts can use this mechanism to set flexible required-verbosity
### levels for their messages.
ZONEMGR_DEBUGLEVEL_NOOP="-1"
# logmsg_echo() adds no prefix and prints the message if ZONEMGR_DEBUG>=$1
# (or ZONEMGR_DEBUGLEVEL_ECHO if $1 is not a number)
default_posval ZONEMGR_DEBUGLEVEL_ECHO       0

# Standard stuff
default_posval ZONEMGR_DEBUGLEVEL_ERROR      1
default_posval ZONEMGR_DEBUGLEVEL_WARN       2
default_posval ZONEMGR_DEBUGLEVEL_INFO       3
default_posval ZONEMGR_DEBUGLEVEL_TRACE      4
default_posval ZONEMGR_DEBUGLEVEL_DEBUG      5

# Custom stuff for specific routines
default_posval ZONEMGR_DEBUGLEVEL_RUN        $ZONEMGR_DEBUGLEVEL_TRACE
default_posval ZONEMGR_DEBUGLEVEL_PIPESNIFFER $ZONEMGR_DEBUGLEVEL_DEBUG

# Semihack used below to enable "set -x" by very large debug level
default_posval ZONEMGR_DEBUGLEVEL_TRACEEXEC  200
default_posval ZONEMGR_DEBUGLEVEL_TIME_RUN 99

### Default debugging/info/warning level for this lifetime of the script
### Messages are printed if their assigned level is at least ZONEMGR_DEBUG
### The default of "3" allows INFO messages to be printed or easily
### suppressed by change to a smaller number. The level of "2" is default
### for warnings and "1" for errors, and a "0" would likely hide most
### such output. "Yes" bumps up a high level to enable even greater debug
### details, while "No" only leaves the default errors and warnings.
case "${ZONEMGR_DEBUG-}" in
    ""|"-")
        ZONEMGR_DEBUG="$ZONEMGR_DEBUGLEVEL_INFO"
        ;;
    [Yy]|[Yy][Ee][Ss]|[Oo][Nn]|[Tt][Rr][Uu][Ee])
        ZONEMGR_DEBUG=99 ;;
    [Nn]|[Nn][Oo]|[Oo][Ff][Ff]|[Ff][Aa][Ll][Ss][Ee])
        ZONEMGR_DEBUG="$ZONEMGR_DEBUGLEVEL_WARN" ;;
esac
[ "$ZONEMGR_DEBUG" -ge -1 ] 2>/dev/null \
    || ZONEMGR_DEBUG="$ZONEMGR_DEBUGLEVEL_INFO"
[ "$ZONEMGR_DEBUG" -ge -1 ] 2>/dev/null \
    || ZONEMGR_DEBUG=3

### This is prefixed before ERROR, WARN, INFO tags in the logged messages
[ -z "$LOGMSG_PREFIX" ] && LOGMSG_PREFIX="ZONEMGR-"

logmsg_echo() {
    ### Optionally echoes a message, based on current debug-level
    ### Does not add any headers to the output line
        # By default, do echo unless $1 says otherwise

    WANT_DEBUG_LEVEL=$ZONEMGR_DEBUGLEVEL_ECHO
    if [ "$1" -ge 0 ] 2>/dev/null; then
        WANT_DEBUG_LEVEL="$1"
        shift
    else if [ x"$1" = x"" ] && [ $# -gt 0 ]; then shift; fi
    fi
    [ "$ZONEMGR_DEBUG" -ge "$WANT_DEBUG_LEVEL" ] 2>/dev/null && \
    echo_E "$@"
    :
}

logmsg_info() {
    WANT_DEBUG_LEVEL=$ZONEMGR_DEBUGLEVEL_INFO
    if [ "$1" -ge 0 ] 2>/dev/null; then
        WANT_DEBUG_LEVEL="$1"
        shift
    else if [ x"$1" = x"" ] && [ $# -gt 0 ]; then shift; fi
    fi
    [ "$ZONEMGR_DEBUG" -ge "$WANT_DEBUG_LEVEL" ] 2>/dev/null && \
    echo_E "${LOGMSG_PREFIX}INFO: ${_SCRIPT_PATH}:" "$@"
    :
}

logmsg_warn() {
    WANT_DEBUG_LEVEL=$ZONEMGR_DEBUGLEVEL_WARN
    if [ "$1" -ge 0 ] 2>/dev/null; then
        WANT_DEBUG_LEVEL="$1"
        shift
    else if [ x"$1" = x"" ] && [ $# -gt 0 ]; then shift; fi
    fi
    [ "$ZONEMGR_DEBUG" -ge "$WANT_DEBUG_LEVEL" ] 2>/dev/null && \
    echo_E "${LOGMSG_PREFIX}WARN: ${_SCRIPT_PATH}:" "$@" >&2
    :
}

logmsg_error() {
    WANT_DEBUG_LEVEL=$ZONEMGR_DEBUGLEVEL_ERROR
    if [ "$1" -ge 0 ] 2>/dev/null; then
        WANT_DEBUG_LEVEL="$1"
        shift
    else if [ x"$1" = x"" ] && [ $# -gt 0 ]; then shift; fi
    fi
    [ "$ZONEMGR_DEBUG" -ge "$WANT_DEBUG_LEVEL" ] 2>/dev/null && \
    echo_E "${LOGMSG_PREFIX}ERROR: ${_SCRIPT_PATH}:" "$@" >&2
    :
}

logmsg_trace() {
    WANT_DEBUG_LEVEL=$ZONEMGR_DEBUGLEVEL_TRACE
    if [ "$1" -ge 0 ] 2>/dev/null; then
        WANT_DEBUG_LEVEL="$1"
        shift
    else if [ x"$1" = x"" ] && [ $# -gt 0 ]; then shift; fi
    fi
    [ "$ZONEMGR_DEBUG" -ge "$WANT_DEBUG_LEVEL" ] 2>/dev/null && \
    echo_E "${LOGMSG_PREFIX}TRACE: ${_SCRIPT_PATH}:" "$@" >&2
    :
}

logmsg_run() {
    WANT_DEBUG_LEVEL=$ZONEMGR_DEBUGLEVEL_RUN
    if [ "$1" -ge 0 ] 2>/dev/null; then
        WANT_DEBUG_LEVEL="$1"
        shift
    else if [ x"$1" = x"" ] && [ $# -gt 0 ]; then shift; fi
    fi
    [ "$ZONEMGR_DEBUG" -ge "$WANT_DEBUG_LEVEL" ] 2>/dev/null && \
    echo_E "${LOGMSG_PREFIX}TRACE-RUN: ${_SCRIPT_PATH}:" "$@" >&2
    :
}

logmsg_debug() {
    # A script can flexibly define its different debug messages via
    # variables with debug-levels assigned (and easily changeable)
    # to different subjects
    WANT_DEBUG_LEVEL=$ZONEMGR_DEBUGLEVEL_DEBUG
    if [ "$1" -ge 0 ] 2>/dev/null; then
        WANT_DEBUG_LEVEL="$1"
        shift
    else if [ x"$1" = x"" ] && [ $# -gt 0 ]; then shift; fi
    fi

    [ "$ZONEMGR_DEBUG" -ge "$WANT_DEBUG_LEVEL" ] 2>/dev/null && \
        for LINE in "$@"; do
            echo_E "${LOGMSG_PREFIX}DEBUG[$WANT_DEBUG_LEVEL<=$ZONEMGR_DEBUG]: $LINE"
        done >&2
    :
}

tee_stderr() {
    ### This routine allows to optionally "sniff" piped streams, e.g.
    ###   prog1 | tee_stderr LISTING_TOKENS $DEBUGLEVEL_PRINTTOKENS | prog2
    TEE_TAG="PIPESNIFF:"
    [ -n "$1" ] && TEE_TAG="$1:"
    [ -n "$2" -a "$2" -ge 0 ] 2>/dev/null && \
        TEE_DEBUG="$2" || \
        TEE_DEBUG=$ZONEMGR_DEBUGLEVEL_PIPESNIFFER

    ### If debug is not enabled, skip tee'ing quickly with little impact
    [ "$ZONEMGR_DEBUG" -lt "$TEE_DEBUG" ] 2>/dev/null && cat || \
    while IFS= read -r LINE; do
        echo_E "$LINE"
        echo_E "${LOGMSG_PREFIX}$TEE_TAG" "$LINE" >&2
    done
    :
}

join_regex() {
    local REGEX=""
    [ $# != 0 ] || return 1
    while [ $# -gt 0 ] ; do
        [ -n "$REGEX" ] && REGEX="$REGEX|$1" || REGEX="$1"
        shift
    done
    echo "($REGEX)"
}

# This value can be set, at least in bash, to specify at which location
# the caller of DIE was in the scripts
SCRIPTLIB_DIE_FILENAME=""   # Which (maybe included) file caused the failure
SCRIPTLIB_DIE_FUNCNAME=""   # Which function failed, maybe main(), source() etc
SCRIPTLIB_DIE_LINENO=0      # Which line in the file (or function) failed?
SCRIPTLIB_DIE_SUBSHELL=-1   # Depth of automatic subshelling "()" "``" "$()"...
# These are inherited from caller in any shell:
SCRIPTLIB_DIE_SCRIPT_PATH=""
SCRIPTLIB_DIE_SCRIPT_NAME=""
SCRIPTLIB_DIE_SCRIPT_ARGS=""
SCRIPTLIB_DIE_SCRIPT_ARGC=-1
# This value is also a flag to decide if the SCRIPTLIB_DIE_* vars are to
# be consulted in the settraps() handler below rather than autodetection
# at the moment of trap execution (which may be already limited by shell).
SCRIPTLIB_DIE_ERRCODE=-1

die() {
    # The exit CODE can be passed as a variable, or as the first parameter
    # (if it is a number), both ways can be used for legacy reasons
    _EXIT_CODE_PREV=$?
    #CODE="${CODE-1}"
    if [ "${1-}" -ge 0 ] 2>/dev/null; then
        CODE="$1"
        shift
    else
        if [ x"$1" = x"" ] && [ $# -gt 0 ]; then shift; fi
    fi
    [ "$CODE" -ge 0 ] 2>/dev/null || CODE="${_EXIT_CODE_PREV}"
    [ "$CODE" -ge 0 ] 2>/dev/null || CODE=1

    for LINE in "$@" ; do
        echo_E "${LOGMSG_PREFIX-}FATAL: ${_SCRIPT_PATH-}:" "$LINE" >&2
    done

    # Set common vars for settraps() standard handler
    SCRIPTLIB_DIE_ERRCODE="${CODE}"
    SCRIPTLIB_DIE_SCRIPT_PATH="${_SCRIPT_PATH-}"
    SCRIPTLIB_DIE_SCRIPT_NAME="${_SCRIPT_NAME-}"
    SCRIPTLIB_DIE_SCRIPT_ARGS="${_SCRIPT_ARGS-}"
    SCRIPTLIB_DIE_SCRIPT_ARGC="${_SCRIPT_ARGC-}"

    if [ -n "${BASH}" ]; then
        # Detect who called die()
        SCRIPTLIB_DIE_LINENO="${BASH_LINENO[0]-}"
        SCRIPTLIB_DIE_FUNCNAME="${FUNCNAME[1]-}"
        SCRIPTLIB_DIE_FILENAME="${BASH_SOURCE[1]}"
        SCRIPTLIB_DIE_SUBSHELL="${BASH_SUBSHELL}"
    else
        # Definitions of LINENO vary greatly from shell to shell
        # In practice must be >= 1 if defined at all
        SCRIPTLIB_DIE_LINENO="${LINENO-}"
        [ "$SCRIPTLIB_DIE_LINENO" -ge 1 ] 2>/dev/null \
            || SCRIPTLIB_DIE_LINENO=0
        SCRIPTLIB_DIE_FILENAME="${SCRIPTLIB_DIE_SCRIPT_NAME}"
        SCRIPTLIB_DIE_FUNCNAME=""
        SCRIPTLIB_DIE_SUBSHELL=-1
    fi

    exit $CODE
}

settraps_exit_clear() {
    # Reset the exit() handler to defaults, used in routines below
    for SS in EXIT SIGEXIT 0 ERR SIGERR; do
        trap "-" "$SS" 2>/dev/null \
            || logmsg_debug "Could not reset signal handler for '$SS'"
    done
    set +E      # unset -o errtrace
}

clean_exit() {
    _EXIT_CODE_PREV=$?
    if [ "${1-}" -ge 0 ] 2>/dev/null; then
        CODE="$1"
        shift
    else
        if [ x"$1" = x"" ] && [ $# -gt 0 ]; then shift; fi
    fi
    [ "$CODE" -ge 0 ] 2>/dev/null || CODE="${_EXIT_CODE_PREV}"
    [ "$CODE" -ge 0 ] 2>/dev/null || CODE=1

    settraps_exit_clear

    for LINE in "$@" ; do
        echo_E "${LOGMSG_PREFIX-}FATAL: ${_SCRIPT_PATH-}:" "$LINE" >&2
    done

    exit $CODE
}

settraps_nonfatal() {
    # While the legacy common settraps() wraps the caller's custom handler
    # "$*" with an exit(), this one is not fatal by itself, and it does not
    # subshell as it is intended to be used for interrupts, etc. And for
    # settraps() too. It detects and presets the variables that can be used
    # by the caller's trap handler, including one from settraps() below.
    # Variables include:
    #   ERRCODE     Number of upstream exitcode that came into the trap
    #   ERRSIGNAL   Name of the signal as registered (HUP or SIGEXIT etc.)
    #   ERRFILE     File from which the trap was called, if we can guess it
    #   ERRFUNC     Function inside which failure, exit() or die() happened
    #   ERRLINE     Line in source file or function, if any (else empty)
    #   ERRPOS      String that combines available bits of _SCRIPT_NAME
    #               ERRFILE ERRFUNC ERRLINE into meaningful markup
    #   ERRTEXT     String that meaningfully combines ERRPOS ERRCODE ERRSIGNAL
    # The handler built into this routine does not report anything, it just
    # sets the variables above and calls the caller's handle - such as the
    # settrap() which reports stuff and exit()s with some code in the end.

    # Not all trap names are recognized by all shells consistently
    [ -z "${TRAP_SIGNALS-}" ] && TRAP_SIGNALS="EXIT QUIT TERM HUP INT ERR"
    for P in "" SIG; do for S in $TRAP_SIGNALS ; do
        if [ -n "$BASH" ] && [ "$S" = ERR ] ; then
            # If "set -e" aka "set -o errexit" would be used, inherit the trap
            set -o errtrace
        fi
        case "$1" in
            -|"") trap "$1" "$P$S" 2>/dev/null \
                || logmsg_debug "Could not set up signal handler for '$P$S'" ;;
            *)    ERRHANDLER="$*"
                  case "$ERRHANDLER" in
                    *";"|*"; "|*";  ") ;;
                    *)    ERRHANDLER="$ERRHANDLER ;" ;;
                  esac
                  trap 'ERRCODE=$?; ERRSIGNAL="'"$P$S"'"; \
[ -z "${ERRIGNORE-}" ] && ERRIGNORE=no
if [ "${ERRSIGNAL-}" = "ERR" ] || [ "${ERRSIGNAL-}" = "SIGERR" ]; then
    set -o | egrep -i "^errexit.*off$" >/dev/null && ERRIGNORE=yes
fi
if [ "${ERRIGNORE-}" = no ]; then
  if [ -n "${SCRIPTLIB_DIE_ERRCODE-}" ] && \
     [ "${SCRIPTLIB_DIE_ERRCODE-}" -ge 0 ] 2>/dev/null; then
    ERRFILE="$SCRIPTLIB_DIE_FILENAME"
    ERRFUNC="$SCRIPTLIB_DIE_FUNCNAME"
    ERRLINE="$SCRIPTLIB_DIE_LINENO"
    ERRCODE="$SCRIPTLIB_DIE_ERRCODE"
  else
    SCRIPTLIB_DIE_ERRCODE=""
    [ -n "${LINENO-}" ] && [ "${LINENO-}" -gt 0 ] 2>/dev/null \
        && ERRLINE="${LINENO-}" || ERRLINE=""
    ERRFILE="${_SCRIPT_NAME-}"; ERRFUNC=""
    if [ -n "${BASH-}" ] 2>/dev/null; then
        [ -n "${FUNCNAME-}" -o -n "${FUNCNAME[0]-}" ] \
            && ERRFUNC="${FUNCNAME[0]-}" || ERRFUNC=""
        ERRLINE="${BASH_LINENO[0]-}" && [ -n "$ERRLINE" ] || ERRLINE=0
        [ "$ERRLINE" -eq 0 ] && ERRLINE="${LINENO-}"
        [ -n "$ERRLINE" ] && [ "$ERRLINE" -gt 1 ] || ERRLINE=""
        ERRFILE="${BASH_SOURCE[0]-}"
    fi
  fi
  ERRPOS="${ERRFILE-}${ERRLINE:+:$ERRLINE}${ERRFUNC:+ :: $ERRFUNC()}"
  [ "`basename "${_SCRIPT_NAME-}"`" = "`basename "${ERRFILE-}"`" ] \
    || ERRPOS="${_SCRIPT_NAME-} => ${ERRPOS-}"
  ERRTEXT="script (${ERRPOS-}) due to trapped signal (${ERRSIGNAL-}) with exit-code (${ERRCODE-})"
  [ -n "${SCRIPTLIB_DIE_ERRCODE-}" ] && ERRTEXT="${ERRTEXT-}, using die()"
  { (settraps_exit_clear; exit ${ERRCODE-} 2>/dev/null 2>&1); '"$ERRHANDLER"' } ;
else ERRIGNORE=""; fi ;' \
                    "$P$S" 2>/dev/null \
                    || logmsg_debug "Could not set up signal handler for '$P$S'"
                  ;;
        esac
    done; done
}

# These variables are tested for equality to "yes" when the trap is processed.
#   When a trap is handled by settraps(), should any message be printed?
[ -n "${SCRIPTLIB_TRAPWRAP_PRINT_MESSAGE-}" ] && \
[ x"${SCRIPTLIB_TRAPWRAP_PRINT_MESSAGE-}" != "x-" ] || \
    if [ "$ZONEMGR_DEBUG" -ge "$ZONEMGR_DEBUGLEVEL_ERROR" ]; then
        SCRIPTLIB_TRAPWRAP_PRINT_MESSAGE="yes"
    else
        SCRIPTLIB_TRAPWRAP_PRINT_MESSAGE="yes"
    fi
#   If SCRIPTLIB_TRAPWRAP_PRINT_MESSAGE==yes and cause is exit(0), still print?
[ -n "${SCRIPTLIB_TRAPWRAP_PRINT_EXIT0-}" ] && \
[ x"${SCRIPTLIB_TRAPWRAP_PRINT_EXIT0-}" != "x-" ] || \
    if [ "$ZONEMGR_DEBUG" -ge "$ZONEMGR_DEBUGLEVEL_DEBUG" ]; then
        SCRIPTLIB_TRAPWRAP_PRINT_EXIT0="yes"
    else
        SCRIPTLIB_TRAPWRAP_PRINT_EXIT0="no"
    fi
#   If SCRIPTLIB_TRAPWRAP_PRINT_MESSAGE==yes and we have a shell with func-call
#   history like bash, print also a stack trace of the failure?
[ -n "${SCRIPTLIB_TRAPWRAP_PRINT_STACKTRACE-}" ] && \
[ x"${SCRIPTLIB_TRAPWRAP_PRINT_STACKTRACE-}" != "x-" ] || \
    if [ "$ZONEMGR_DEBUG" -ge "$ZONEMGR_DEBUGLEVEL_DEBUG" ]; then
        SCRIPTLIB_TRAPWRAP_PRINT_STACKTRACE="yes"
    else
        SCRIPTLIB_TRAPWRAP_PRINT_STACKTRACE="no"
    fi

settraps() {
    # Sets up or clear traps defined in $TRAP_SIGNALS (or falls back to
    # default list of signals) to report the trap, call consumer's handler,
    # and if that routine does not exit() the shell by itself - the wrapper
    # would exit with either that handler's non-zero return code or with
    # original trapped code. The ERR* variables reported here are defined
    # by settraps_nonfatal() above. Note that all output (if any) goes to
    # stderr (see end of "if" clause).
    case "$1" in
        -|"") settraps_nonfatal "$1" \
            || logmsg_warn "Could not settraps_nonfatal()" ;;
        *)    ERRHANDLER="$*"
              case "$ERRHANDLER" in
                *";"|*"; "|*";  ") ;;
                *)    ERRHANDLER="$ERRHANDLER ;" ;;
              esac
              settraps_nonfatal 'if [ "${SCRIPTLIB_TRAPWRAP_PRINT_MESSAGE-}" = yes ]\
; then
    echo ""
    _DO_PRINT_STACKTRACE=no
    if [ "${SCRIPTLIB_TRAPWRAP_PRINT_STACKTRACE-}" = yes ] && \
       [ -n "${BASH-}" ]; then
        _DO_PRINT_STACKTRACE=yes
    fi
    if [ "${ERRCODE-}" = 0 ]; then
        _DO_PRINT_STACKTRACE=no
        if [ "${SCRIPTLIB_TRAPWRAP_PRINT_EXIT0-}" = yes ] || \
            [ "${ERRSIGNAL-}" != 0 -a "${ERRSIGNAL-}" != EXIT \
              -a "${ERRSIGNAL-}" != SIGEXIT ] \
        ; then
            LOGMSG_PREFIX="ZONEMGR-SIGNALTRAP-" logmsg_info "Completing ${ERRTEXT-}"
            _DO_PRINT_STACKTRACE=yes
        fi
    else
        echo ""; echo "!!!!!!!!!"
        LOGMSG_PREFIX="ZONEMGR-SIGNALTRAP-" logmsg_error "Aborting ${ERRTEXT-}"
        echo "!!!!!!!!!"
    fi
    echo ""
    if [ "${_DO_PRINT_STACKTRACE-}" = yes ]; then
        echo "======= Stack trace and other clues of the end-of-work (code=${ERRCODE-}, sig=${ERRSIGNAL-}):"
        echo "  Depth of sub-shelling (BASH_SUBSHELL) = ${BASH_SUBSHELL-}"
        if [ -z "${FUNCNAME-}" ] || [ -z "${FUNCNAME[0]}" ]; then
            FUNCDEPTH=-1
        else
            FUNCDEPTH="${#FUNCNAME[@]-}" 2>/dev/null && \
                [ -n "$FUNCDEPTH" ] && [ "$FUNCDEPTH" -ge 0 ] \
            || FUNCDEPTH=-1
        fi
        printf "  Depth of function call stack = $FUNCDEPTH : "
        if [ "$FUNCDEPTH" -gt 0 ] 2>/dev/null; then
            printf "::%s" ${FUNCNAME[@]-}
        else
            printf "finished in main body of main script"
        fi
        printf "\n"
        i=0
        while [ "$i" -lt "$FUNCDEPTH" ] ; do
            echo "  ($i)        -> in ${FUNCNAME[$i]-}() called at ${BASH_SOURCE[$i+1]-}:${BASH_LINENO[$i]-}"
            i=$(($i+1))
        done
        echo "  ~> in ${ERRFUNC:-main-script-body}() at ${ERRFILE-}:${ERRLINE-}"
        echo "======= End of stack trace, ${_SCRIPT_NAME-}:${LINENO-}"
        echo ""
    fi
fi >&2
settraps_exit_clear
{ '"$ERRHANDLER"' } || exit $?
exit ${ERRCODE-};' \
                || logmsg_warn "Could not settraps_nonfatal()"
              ;;
    esac
}

exit_cleanup() {
    :
}

TBD() {
    if [ -n "${FUNCNAME[1]}" ]; then
        logmsg_error "Routine ${FUNCNAME[1]}() is not implemented yet"
    else
        ( die "This routine is not implemented yet" )
    fi
    false
}

##########################################################################
# Slurp script configs (hardcoded defaults; system, user level and
# CLI-passed config file overrides) - e.g. container root datasets
##########################################################################

# TODO: Add (and rename?) options for things supported by zone-brand
# scripts commonly, such as installation of a zone filesystem from
# archives and ZFS snapshots instead of packaging.
# TODO: Support zone naming by UUID (zoneadm -u uuid), not only by
# name (-z) which we already do support?
# TODO: Add support for changing configuration of an existing zone
# without making a clone with amended config? Limit this to just some
# changes?
# TODO: Clarify usage of brands vs. templates vs. cloning... verify the
# latter are not aspects of the same functionality? :)
usage() {
    cat << EOF
Usage: ${_SCRIPT_NAME} (options...) (action)
Options: if conflicting options are specified, then the last one wins
(except when setting defaults for certain actions).
    -h|--help           Display this help text and exit
    --config FILE       Use a specified configuration file after parsing
            system and user-homedir and \$ZONEMGR_CONF default files and
            before parsing any subsequent command-line options
    --print-config      Show active configuration after parsing CLI args
    -d|--debug LEVEL    Set debugging level to the specified value
    -q|-Q               Set debug to warning+error (-q) or only error (-Q)
    -O|--outfile NAME   Write results of the work into the output file,
            e.g. record the created (generated) zone name there
    -n|--dry-run        Do not create/destroy/change datasets and zones,
            but only print what would be done instead
    -t|--template NAME  Use this OS-defined template to create a new GIZ
    -b|--brand NAME     Record this brand in the zone attributes
    -c|--clone|--origin NAME    The origin GIZ for cloning into new GIZ or DIZ
    -s|--snapshot NAME  The snapshot to take from target or use from origin,
            or rollback to (or destroy-snapshot) on target
    -z|--zone NAME      Target zone to create or otherwise manage
    -Z|--zone-autoname  Generate name for the zone as a hash of requested
            package names. When asked to create-diz or exec-diz,
            the script would find or create (for future reuse)
            a suitable source GIZ (maybe starting as a clone of
            specified origin, or from template)
    --exec-cmd SCRIPT   Execute the command in the zone via zlogin
    --exec-user NAME    ...as the specified LZ-defined user account (or -S)
    --exec-discard off|always|onsuccess    Discard the zone after running the
            command? Defaults to "off" usually, and to "always"
            for "exec-diz" action

Add LZ network interfaces as dedicated VNICs (exclusive IP mode) that will
be DHCP enabled (the GZ may serve DHCP+NAT over an etherstub for example):
    --add-vnic NAME     Assign an (additional) existing VNIC to the zone
    --add-vnic-over LINK    Temporarily create a VNIC over the specified
            link (e.g. a physical NIC or an etherstub) - it will
            be removed when the zone is destroyed by this script.
            To use advanced setup, specify LINK as pipe-separated
            tuple of "LINK[|VLAN[|MAC]]" where unspecified values
            are assumed as OS-default setting.
            The vnic will be pre-set for legacy DHCP (/etc/dhcp.VNICNAME).
            TODO: Add an option for more flexible config?
    TODO: Add options to define desired VNIC name and/or use existing NICs?

The following options APPEND to an existing list (so you can specify them
several times):
    -p|--add-packages LIST      Whitespace-separated list of package FMRIs to
            install when creating the zone, or to manage later
    -P|--remove-packages LIST   Whitespace-separated list of package FMRIs to
            uninstall when creating the zone, or to manage later
    -g|--add-publisher LIST     Add the specified publisher(s) when creating
            the zone or later; WS-SEP list of "pub|url" items. If this list
            is not empty, the first publisher will be the default one to seed
            the basic OS in the local zone (per host definition of its brand).
            Other publishers and packages will be added after initial install.
    -G|--remove-publisher LIST  Remove publisher URLs and/or definitions using
            same argument format as above
    --mount-lofs LIST           Set up lofs-mounting of specified paths from
            GZ, as a whitespace-separated list of "lzpath[|gzpath]" items
    --mount-autonfs LIST        Set up autofs-mounting of specified NFS paths,
            as whitespace-separated list of "lzpath|NFSurl" items
    --delegate-dataset LIST     Set up creation and delegation of specified
            whitespace-separated list of zfs datasets (usage to be defined by
            corresponding local zone admin)
    --copy-users LIST   Copy listed (white-space separated) group and user
    --copy-groups LIST  ...account defs from GZ to LZ (lines in file DBs)
    --elevate-users LIST        Set these accounts for RBAC and sudo like root

Actions: the last specified action is performed (so please only call one).
    create-giz          Create target golden-image zone from template/origin
            (configure, install, disable auto-boot)
    create-diz          Create target disposable-image zone from origin GIZ
            (configure, install, enable auto-boot and boot up)
    exec-diz            Creates by default an auto-named zone (see "-Z" above)
            with settings specified by CLI and/or custom config,
            runs a command in this zone, and discards the zone
    exec                Using an existing zone, runs a command in it (boots
            it up if needed - and halts afterwards in this case)
    list|list-diz|list-giz    List zones managed by this script, accepts a
            single optional parameter with valid mix of zoneadm tokens -cpiv
    is-managed|is-diz|is-giz (NAME)     Query if the named zonename is managed
            by this script (name can be passed quickly without -z)
    get-state (NAME)    Report current state of a zone
    verify (NAME)       Query if the named zonename has valid manifest and
            is managed by this script
    destroy             Uninstall and unconfigure the specified zone, and
            destroy any automatically created VNICs, requires "-z ZONENAME"
    boot|start          Just boot the specified zone, or also wait for its
            multi-user-server milestone to get reached
    singleuser          Boot the zone to single-user mode
    stop|halt|poweroff  Halt or quickly shutdown the specified zone
    shutdown            Gracefully shutdown the specified zone
    restart|reboot      Reboot the specified zone
    mount|ready|unmount Change FS state of the specified zone filesystems
    pkg-refresh         Fetch the updated packaging metadata
    pkg-upgrade         Upgrade packages installed in this zone (no refresh)
    pkg-install         Install packages specified in "-p" list
    pkg-uninstall       Uninstall packages specified in "-P" list
    snapshot            (Shutdown and?) Take a snapshot of target zone
                        (requires -z and optional -s arguments)
    rollback            (Halt and) roll back the target zone to old snapshot
                        (requires -z and optional -s arguments)
    list-snapshots (name)      List snapshots (the @comments) of the target
            zone (name can be passed quickly without -z)

NOTE: The user account running this script should be permitted to elevate
privileges for certain operations. You can use config files to use "sudo",
or better stick with default "pfexec" and set up Solaris RBAC for the account:
    usermod -P "File System Management,Object Access Management,Media Backup,Media Restore,ZFS File System Management,ZFS Storage Management,Zone Management,Zone Security,Network Management" $USER
EOF
}

if [ $# = 0 ] && [ -z "${ZONEMGR_IS_INCLUDED-}" ] ; then
    echo "ERROR: No (valid) required arguments were provided" >&2
    usage
    exit 1
fi


# Track if the script is called by root or mere mortals
MY_UID="`id -u 2>/dev/null`" || MY_UID=-1

# Prefix commands that change system state with this
ELEVATE_CMD="pfexec"
OUTFILE=""

# Golden images live under this dataset
DATASET_CONTAINMENT_GIZ="rpool/zones/_zonemgr/GIZ"
# Disposable zones live under this dataset
DATASET_CONTAINMENT_DIZ="rpool/zones/_zonemgr/DIZ"
# Mountpoints for the above. If populated before creation, causes script to
# set the "mountpoint" option of the dataset. If empty, the default is used.
# After passing the routines to ensure presence of the datasets, the _MPT
# values are reinitialized to actual value of respective mountpoint attr.
DATASET_CONTAINMENT_GIZ_MPT=""
DATASET_CONTAINMENT_DIZ_MPT=""
# TODO: Add support (routines, CLI) for automated lifecycle of delegated
# datasets stored in their delegated ZFS container, and perhaps stored in
# a different pool. It would still be a job for the local zone to create,
# mount and use datasets inside it (might start with a standard mountpoint
# though).
# TODO: For read-only serving, support cloning datasets from GZ into the
# zone, so anything that goes bad does not impact the real data outside.

# Flip this to "yes" in a config to use "zfs create -p"
CAN_ZFS_CREATE_RECURSIVE=no
# Flip this to "on" in a config to dedup under the GIZ container
# This defaults to "off" based on theory that detection of a zone to clone
# and so inherit its contents would work, so anything that intersects is
# already referenced per cloning. This assumption can fail for zones made
# from multiple sources (e.g. development of new packages) where dedup is
# unlikely to help, but dedup may help with different brands of zones
# coming from same or similar sources, or as the installed zones deviate
# due to package updates - installing same files into different longterm BE's.
DATASET_CONTAINMENT_GIZ_DEDUP=off
# For GIZ we want strong compression, it is not written often
DATASET_CONTAINMENT_GIZ_COMPRESSION="gzip-9"
# For DIZ clones we want fast compression, like "on" or "lz4" or "zle"
# and no dedup since these zones are mostly clones of existing golden images
DATASET_CONTAINMENT_DIZ_COMPRESSION="zle"
DATASET_CONTAINMENT_DIZ_DEDUP=off

# Which zone template a GIZ from scratch starts with
TEMPLATE_GIZ="SUNWipkg"
BRAND_GIZ="ipkg"
# Which golden-image zone the target one (GIZ or DIZ) is cloned from
ORIGIN_GIZ=""
# Which zone will we manage with this command?
TARGET_ZONE=""
# Which snapshot name will we take of target zone, or clone off origin zone,
# or rollback to? If empty, then for snapshot this is generated by current
# timestamp, and for clone/rollback it is the newest snapshot (last in list).
# TODO: Maybe add parsing of ZONENAME@SNAPNAME syntax into the two variables?
SNAP_NAME=""
# TODO: Add a -F to enforce some operations, e.g. rollback (and halt) of a
# running zone? Or rely on OS tools to fail and so require several operations
# after the failure code... is this too not-friendly)?

ADD_PUBLISHERS=""
DEL_PUBLISHERS=""
ADD_PACKAGE_FMRIS=""
DEL_PACKAGE_FMRIS=""
ADD_MOUNT_LOFS=""
ADD_MOUNT_AUTONFS=""
ADD_DELEGATED_DATASET=""
COPY_USERS=""
COPY_GROUPS=""
ELEVATE_USERS=""
EXEC_CMD=""
EXEC_USER=""
EXEC_DISCARD=""
USE_VNICS=""
AUTO_VNICS=""

# Arguments for zone-listing (passed to zoneadm)
LIST_ARGS=""

# What will we do?
ACTION=""
ZONECFG_SCRIPT=""

timestamp_now() {
    TZ=UTC date -u "+%Y%m%dT%H%M%SZ"
}

parse_config_file() {
    ### Note: the config should end with a "true" evaluation, e.g. a ":" line
    logmsg_info "Reading config file: $1"
    [ -f "$1" ] && . "$1" || return $?
}

parse_config_args() {
    logmsg_debug "Parsing config args:" "$@"
    while [ "$#" -gt 0 ]; do
        case "$1" in
            -h|-help|--help) usage ; exit 0 ;;
            DEVEL_DEBUG)
                shift 1

                if [ -z "$DATASET_CONTAINMENT_GIZ_MPT" ]; then
                    DATASET_CONTAINMENT_GIZ_MPT="$(zfs_getMPTbyDS "$DATASET_CONTAINMENT_GIZ")" \
                        && [ -d "$DATASET_CONTAINMENT_GIZ_MPT" ] \
                        || logmsg_warn "Not a directory: DATASET_CONTAINMENT_GIZ_MPT='$DATASET_CONTAINMENT_GIZ_MPT'"
                fi

                if [ -z "$DATASET_CONTAINMENT_DIZ_MPT" ]; then
                    DATASET_CONTAINMENT_DIZ_MPT="$(zfs_getMPTbyDS "$DATASET_CONTAINMENT_DIZ")" \
                        && [ -d "$DATASET_CONTAINMENT_DIZ_MPT" ] \
                        || logmsg_warn "Not a directory: DATASET_CONTAINMENT_DIZ_MPT='$DATASET_CONTAINMENT_DIZ_MPT'"
                fi

                logmsg_info "DEBUG-executing a single procedure: $*" >&2
                ( set -x ; [ $# -gt 0 ] && "$@" )
                RES_DEBUG=$?
                if [ "$RES_DEBUG" != 0 ]; then
                    logmsg_error "FAILED with code $RES_DEBUG while running a single procedure: $*" >&2
                fi
                exit $RES_DEBUG
                ;;
            --config)
                parse_config_file "$2" || exit $?
                shift
                ;;
            --print-config) DO_PRINT_CONFIG=1 ;;
            -q) ZONEMGR_DEBUG="$ZONEMGR_DEBUGLEVEL_WARN" ;;
            -Q) ZONEMGR_DEBUG="$ZONEMGR_DEBUGLEVEL_ERROR" ;;
            -d|--debug)
                if [ "$2" -ge -1 ] 2>/dev/null ; then
                    ZONEMGR_DEBUG="$2"
                    shift
                else
                    ZONEMGR_DEBUG="$(expr $ZONEMGR_DEBUG + 1)" || \
                        ZONEMGR_DEBUG="$ZONEMGR_DEBUGLEVEL_DEBUG"
                fi
                ;;
            -O|--outfile) OUTFILE="$2"; shift ;;
            -n|--dryrun|--dry-run) ELEVATE_CMD="echo === DRY-RUN: $ELEVATE_CMD" ;;
            -t|--template) TEMPLATE_GIZ="$2"; shift ;;
            -b|--brand) BRAND_GIZ="$2"; shift ;;
            -c|--clone|--origin) ORIGIN_GIZ="$2"; shift; [ -n "$ACTION" ] || ACTION="create-diz" ;;
            -s|--snapshot|--snapname) SNAP_NAME="$2"; shift ;;
            -z|--zone) TARGET_ZONE="$2"; shift ;;
            -Z|--zone-autoname) TARGET_ZONE='{auto}' ;;
            -p|--add-packages|--install-packages|--add-package|--install-package)
                ADD_PACKAGE_FMRIS="$ADD_PACKAGE_FMRIS $2"; shift ;;
            -P|--remove-packages|--del-packages|--uninstall-packages|--remove-package|--del-package|--uninstall-package)
                DEL_PACKAGE_FMRIS="$DEL_PACKAGE_FMRIS $2"; shift ;;
            -g|--add-publisher|--add-publishers)
                ADD_PUBLISHERS="$ADD_PUBLISHERS $2"; shift ;;
            -G|--remove-publisher|--del-publisher|--remove-publishers|--del-publishers)
                DEL_PUBLISHERS="$DEL_PUBLISHERS $2"; shift ;;
            --mount-lofs)
                ADD_MOUNT_LOFS="$ADD_MOUNT_LOFS $2"; shift ;;
            --mount-autonfs)
                ADD_MOUNT_AUTONFS="$ADD_MOUNT_AUTONFS $2"; shift ;;
            --delegate-dataset)
                ADD_DELEGATED_DATASET="$ADD_DELEGATED_DATASET $2"; shift ;;
            --copy-users|--copy-user)
                COPY_USERS="$COPY_USERS $2"; shift ;;
            --copy-groups|--copy-group)
                COPY_GROUPS="$COPY_GROUPS $2"; shift ;;
            --elevate-users|--elevate-user)
                ELEVATE_USERS="$ELEVATE_USERS $2"; shift ;;
            --add-vnic|--use-vnic|--add-vnics|--use-vnics)
                USE_VNICS="$USE_VNICS $2"; shift ;;
            --add-vnic-over|--use-vnic-over|--add-vnics-over|--use-vnics-over|--auto-vnic|--auto-vnics)
                AUTO_VNICS="$AUTO_VNICS $2"; shift ;;
            --exec-cmd) EXEC_CMD="$2" ; shift ;;
            --exec-user) EXEC_USER="$2" ; shift ;;
            --exec-discard)
                case "$2" in
                    off|always|onsuccess) EXEC_DISCARD="$2"; shift ;;
                    *) logmsg_error "Unknown CLI argument: $1 $2" ; exit 1 ;;
                esac
                ;;
            create-giz|create-diz|destroy|snapshot|rollback|pkg-refresh|pkg-upgrade|pkg-install|pkg-uninstall) ACTION="$1" ;;
            destroy-snapshot|destroy-snap) ACTION=destroy-snapshot ;;
            list|list-giz|list-diz|list-all)
                [ "$1" = list-all ] && ACTION="list" || ACTION="$1"
                case "$2" in
                    -c|-v|-cv|-vc|-cp|-p|-pv|-i|-iv|-vi|-ip|pi)  LIST_ARGS="$2" ; shift ;;
                esac
                ;;
            list-snapshots|list-snaps) ACTION="list-snapshots"
                [ -n "$TARGET_ZONE" ] || \
                case "$2" in
                    -*|"") ;;
                    *)  TARGET_ZONE="$2" ; shift ;;
                esac
                ;;
            is-managed|is-managed-all|is-managed-any) ACTION="is-managed-any"
                [ -n "$TARGET_ZONE" ] || \
                case "$2" in
                    -*|"") ;;
                    *)  TARGET_ZONE="$2" ; shift ;;
                esac
                ;;
            is-diz|is-managed-diz) ACTION="is-managed-diz"
                [ -n "$TARGET_ZONE" ] || \
                case "$2" in
                    -*|"") ;;
                    *)  TARGET_ZONE="$2" ; shift ;;
                esac
                ;;
            is-giz|is-managed-giz) ACTION="is-managed-giz"
                [ -n "$TARGET_ZONE" ] || \
                case "$2" in
                    -*|"") ;;
                    *)  TARGET_ZONE="$2" ; shift ;;
                esac
                ;;
            verify|validate) ACTION="verify"
                [ -n "$TARGET_ZONE" ] || \
                case "$2" in
                    -*|"") ;;
                    *)  TARGET_ZONE="$2" ; shift ;;
                esac
                ;;
            get-state|getstate|state|status) ACTION="get-state"
                [ -n "$TARGET_ZONE" ] || \
                case "$2" in
                    -*|"") ;;
                    *)  TARGET_ZONE="$2" ; shift ;;
                esac
                ;;
            exec-diz|run-diz) ACTION="exec-diz"
                [ -n "$EXEC_DISCARD" ] || EXEC_DISCARD="always"
                [ -n "$TARGET_ZONE" ] || TARGET_ZONE='{auto}'
                ;;
            exec|run) ACTION="exec" ;;
            singleuser|boot-single) ACTION="singleuser" ;;
            start|boot|stop|halt|poweroff|shutdown|restart) ACTION="$1" ;;
            reboot|reset) ACTION="reboot" ;;
            ready|mount) ACTION="$1" ;;
            unmount|umount) ACTION="unmount" ;;
            *) logmsg_error "Unknown CLI argument: $1" ; exit 1 ;;
        esac
        shift
    done
    [ -n "$EXEC_DISCARD" ] || EXEC_DISCARD="off"
}

print_config() {
    logmsg_info "Currently active configuration variables of ${_SCRIPT_NAME} ${_SCRIPT_ARGS}:" \
    "DATASET_CONTAINMENT_GIZ='$DATASET_CONTAINMENT_GIZ'" \
    "DATASET_CONTAINMENT_GIZ_MPT='$DATASET_CONTAINMENT_GIZ_MPT'" \
    "DATASET_CONTAINMENT_GIZ_DEDUP='$DATASET_CONTAINMENT_GIZ_DEDUP'" \
    "DATASET_CONTAINMENT_GIZ_COMPRESSION='$DATASET_CONTAINMENT_GIZ_COMPRESSION'" \
    "DATASET_CONTAINMENT_DIZ='$DATASET_CONTAINMENT_DIZ'" \
    "DATASET_CONTAINMENT_DIZ_MPT='$DATASET_CONTAINMENT_DIZ_MPT'" \
    "DATASET_CONTAINMENT_DIZ_DEDUP='$DATASET_CONTAINMENT_DIZ_DEDUP'" \
    "DATASET_CONTAINMENT_DIZ_COMPRESSION='$DATASET_CONTAINMENT_DIZ_COMPRESSION'" \
    "CAN_ZFS_CREATE_RECURSIVE='$CAN_ZFS_CREATE_RECURSIVE'" \
    "TARGET_ZONE='$TARGET_ZONE'" \
    "ORIGIN_GIZ='$ORIGIN_GIZ'" \
    "TEMPLATE_GIZ='$TEMPLATE_GIZ'" \
    "ADD_PUBLISHERS='$ADD_PUBLISHERS'" \
    "DEL_PUBLISHERS='$DEL_PUBLISHERS'" \
    "ADD_PACKAGE_FMRIS='$ADD_PACKAGE_FMRIS'" \
    "DEL_PACKAGE_FMRIS='$DEL_PACKAGE_FMRIS'" \
    "ADD_MOUNT_LOFS='$ADD_MOUNT_LOFS'" \
    "ADD_MOUNT_AUTONFS='$ADD_MOUNT_AUTONFS'" \
    "ADD_DELEGATED_DATASET='$ADD_DELEGATED_DATASET'" \
    "COPY_USERS='$COPY_USERS'" \
    "COPY_GROUPS='$COPY_GROUPS'" \
    "ELEVATE_USERS='$ELEVATE_USERS'" \
    "USE_VNICS='$USE_VNICS'" \
    "AUTO_VNICS='$AUTO_VNICS'" \
    "EXEC_CMD='$EXEC_CMD'" \
    "EXEC_USER='$EXEC_USER'" \
    "EXEC_DISCARD='$EXEC_DISCARD'" \
    "OUTFILE='$OUTFILE'" \
    "ELEVATE_CMD='$ELEVATE_CMD'" \
    "ACTION='$ACTION'"
}

parse_config_startup() {
    # Args: pass "$@" of the script
    for F in "/etc/zones/zonemgr.conf" "${HOME}/.zonemgr.conf" "${ZONEMGR_CONF-}"; do
        if [ -s "$F" ] ; then
            parse_config_file "$F" \
                || logmsg_warn "Could not use config file '$F'"
        fi
    done

    DO_PRINT_CONFIG=0
    parse_config_args "$@"
    if [ "$DO_PRINT_CONFIG" = 1 ] || \
       [ "$ZONEMGR_DEBUG" -gt "$ZONEMGR_DEBUGLEVEL_INFO" ] \
    ; then
        print_config
    fi

    ZONECFG_SCRIPT=""
}

##########################################################################
# Zone-creation task stack:
##########################################################################

zfs_getDSbyMPT() {
    local MPT="$1"
    local N
    local M
    [ -n "$MPT" ] || return 1

    zfs list -H -o name,mountpoint | \
        while read N M ; do
            [ "$M" = "$MPT" ] && echo "$N" && return 0
        done

    return 1
}

zfs_getMPTbyDS() {
    local DSNAME="$1"
    [ -n "$DSNAME" ] || return 1
    zfs list -H -o mountpoint "$DSNAME"
}

zfs_dataset_exists() {
    local DSNAME="$1"
    test "$(zfs list -H -o name "$DSNAME" 2>/dev/null)" = "$DSNAME" && return 0
    local OUT
    OUT="$(zfs list "$DSNAME" 2>/dev/null)" && [ -n "$OUT" ] && return 0
    return 1
}

is_mounted() {
    # $1 can be either DS or MPT, and technically any other string in `mount`
    mount | egrep "(^$1 | $1 )" > /dev/null
}

zfs_create_recursive() {
    # Newer "zfs" command is able to create parent datasets; older ones can't.
    # Last arg is assumed to be dataset name; passes other args to the last
    # called (in loop) "zfs" command (so e.g. provided zfs options are applied
    # only to the target dataset).
    [ "$#" -gt 0 ] \
        && TARGET_DS="$(while [ "$#" -gt 1 ]; do shift; done; echo "$1")" \
        || TARGET_DS=""

    case "$TARGET_DS" in
        ""|-*|/*|*/) die "Bad args to zfs_create_recursive()" ;;
        */*) TARGET_POOL="$(echo "$TARGET_DS" | sed -e 's,^\([^/]*\)/.*,\1,')"
            if ! zfs_dataset_exists "$TARGET_POOL" ] || \
               ! zpool list "$TARGET_POOL" > /dev/null 2>&1 \
            ; then
                die "zfs_create_recursive() requested for '$TARGET_DS' in non-existent pool '$TARGET_POOL'"
            fi
            ;;
        *)  die "Bad args to zfs_create_recursive()" ;;
    esac

    if zfs_dataset_exists "$TARGET_DS" ; then
        logmsg_debug "$TARGET_DS already exists, nothing to do for zfs_create_recursive()"
        # Note: we do not apply requested -o(ptions) to pre-existing dataset
        return 0
    fi

    if [ "$CAN_ZFS_CREATE_RECURSIVE" = yes ]; then
        $ELEVATE_CMD zfs create -p "$@"
        return $?
    else
        # Create parents, recursively - with no extra options
        zfs_create_recursive "$(dirname "$TARGET_DS")" || return $?
        # Finally (in top call) create the dataset maybe with extra options
        $ELEVATE_CMD zfs create "$@"
        return $?
    fi
}

create_dataset_containment_GIZ() {
    # Create the golden-image containment dataset if missing yet
    # * Disable auto-snaps
    # * Disable atime (writes during reads)
    # * Enable strong compression
    # * Enable dedup?
    if zfs_dataset_exists "$DATASET_CONTAINMENT_GIZ" ; then
        logmsg_debug "$DATASET_CONTAINMENT_GIZ already exists, nothing to do for create_dataset_containment_GIZ()"
        # Note: we do not apply custom options to pre-existing dataset
        return 0
    fi

    logmsg_info "Trying to create GIZ container '$DATASET_CONTAINMENT_GIZ'"
    # Newer versions might get away with "zfs create -o k=v [...] dataset"
    # but older ones do not have this luxury - so we portably do it as a
    # chain of commands.
    # TODO: Wrap into "zfs_create_recursive -o ..." with a flag variable.
    zfs_create_recursive "$DATASET_CONTAINMENT_GIZ" \
        || die "create_dataset_containment_GIZ(): Could not create '$DATASET_CONTAINMENT_GIZ'"

    zfs_dataset_exists "$DATASET_CONTAINMENT_GIZ" \
        || die "create_dataset_containment_GIZ(): '$DATASET_CONTAINMENT_GIZ' does not exist"

    logmsg_info "Trying to set ZFS attributes for GIZ container '$DATASET_CONTAINMENT_GIZ'"
    $ELEVATE_CMD zfs set atime=off "$DATASET_CONTAINMENT_GIZ"
    $ELEVATE_CMD zfs set compression="$DATASET_CONTAINMENT_GIZ_COMPRESSION" "$DATASET_CONTAINMENT_GIZ"
    $ELEVATE_CMD zfs set com.sun:auto-snapshot=false "$DATASET_CONTAINMENT_GIZ" || \
        logmsg_warn "Could not set ZFS props: auto-snapshot on '$DATASET_CONTAINMENT_GIZ'"
    case "$DATASET_CONTAINMENT_GIZ_DEDUP" in
        on|off)
            $ELEVATE_CMD zfs set dedup="$DATASET_CONTAINMENT_GIZ_DEDUP" "$DATASET_CONTAINMENT_GIZ" || \
                logmsg_warn "Could not set ZFS props: dedup on '$DATASET_CONTAINMENT_GIZ'" ;;
        *)  logmsg_warn "Unknown dedup setting DATASET_CONTAINMENT_GIZ_DEDUP='$DATASET_CONTAINMENT_GIZ_DEDUP'" ;;
    esac
    if [ -n "$DATASET_CONTAINMENT_GIZ_MPT" ]; then
        $ELEVATE_CMD zfs set mountpoint="$DATASET_CONTAINMENT_GIZ_MPT" "$DATASET_CONTAINMENT_GIZ" || \
            logmsg_warn "Could not set ZFS props: mountpoint on '$DATASET_CONTAINMENT_GIZ'"
    fi
    $ELEVATE_CMD zfs set canmount=on "$DATASET_CONTAINMENT_GIZ"
    $ELEVATE_CMD zfs mount "$DATASET_CONTAINMENT_GIZ" || \
        logmsg_warn "Could not mount ZFS dataset: '$DATASET_CONTAINMENT_GIZ'"

    # Report mount status in log and exit code
    mount | grep "$DATASET_CONTAINMENT_GIZ"
}

create_dataset_containment_DIZ() {
    # Create the disposable-zone containment dataset if missing yet:
    # * Disable auto-snaps
    # * Enable moderate or no compression
    # * Disable dedup
    if zfs_dataset_exists "$DATASET_CONTAINMENT_DIZ" ; then
        logmsg_debug "$DATASET_CONTAINMENT_DIZ already exists, nothing to do for create_dataset_containment_DIZ()"
        # Note: we do not apply custom options to pre-existing dataset
        return 0
    fi

    logmsg_info "Trying to create DIZ container '$DATASET_CONTAINMENT_DIZ'"
    zfs_create_recursive "$DATASET_CONTAINMENT_DIZ" \
        || die "create_dataset_containment_DIZ(): Could not create '$DATASET_CONTAINMENT_DIZ'"

    zfs_dataset_exists "$DATASET_CONTAINMENT_GIZ" \
        || die "create_dataset_containment_DIZ(): '$DATASET_CONTAINMENT_DIZ' does not exist"

    logmsg_info "Trying to set ZFS attributes for DIZ container '$DATASET_CONTAINMENT_DIZ'"
    $ELEVATE_CMD zfs set atime=off "$DATASET_CONTAINMENT_DIZ"
    $ELEVATE_CMD zfs set compression="$DATASET_CONTAINMENT_DIZ_COMPRESSION" "$DATASET_CONTAINMENT_DIZ"
    $ELEVATE_CMD zfs set com.sun:auto-snapshot=false "$DATASET_CONTAINMENT_DIZ" || \
        logmsg_warn "Could not set ZFS props: auto-snapshot on '$DATASET_CONTAINMENT_DIZ'"
    case "$DATASET_CONTAINMENT_DIZ_DEDUP" in
        on|off)
            $ELEVATE_CMD zfs set dedup="$DATASET_CONTAINMENT_DIZ_DEDUP" "$DATASET_CONTAINMENT_DIZ" || \
                logmsg_warn "Could not set ZFS props: dedup on '$DATASET_CONTAINMENT_DIZ'" ;;
        *)  logmsg_warn "Unknown dedup setting DATASET_CONTAINMENT_DIZ_DEDUP='$DATASET_CONTAINMENT_DIZ_DEDUP'" ;;
    esac
    if [ -n "$DATASET_CONTAINMENT_DIZ_MPT" ]; then
        $ELEVATE_CMD zfs set mountpoint="$DATASET_CONTAINMENT_DIZ_MPT" "$DATASET_CONTAINMENT_DIZ" || \
            logmsg_warn "Could not set ZFS props: mountpoint on '$DATASET_CONTAINMENT_DIZ'"
    fi
    $ELEVATE_CMD zfs set canmount=on "$DATASET_CONTAINMENT_DIZ"
    $ELEVATE_CMD zfs mount "$DATASET_CONTAINMENT_DIZ" || \
        logmsg_warn "Could not mount ZFS dataset: '$DATASET_CONTAINMENT_DIZ'"

    # Report mount status in log and exit code
    mount | grep "$DATASET_CONTAINMENT_DIZ"
}

# Mounted filesystem path to get the (target) zone's current rootfs
# Value for the zone manipulated in this run is cached after first call
ALTROOT=""
getALTROOT() {
    [ -z "$ALTROOT" ] || { echo "$ALTROOT"; return 0; }

    local ZONENAME="${1-}"
    if [ -z "$ZONENAME" ] ; then
        ZONENAME="${TARGET_ZONE}"
    fi
    ALTROOT="`get_zone_root "$ZONENAME"`/root" && [ -n "$ALTROOT" ] || return
    echo "$ALTROOT"
}

list_zone_snapshot_datasets() {
    local ZONENAME="$1"
    local SNAPNAME
    check_is_managed_zone "$ZONENAME" || return

    local ZONEROOT_MPT="`get_zone_root "${ZONENAME}"`" || return
    local ZONEROOT_DS="`zfs_getDSbyMPT "${ZONEROOT_MPT}"`" || return

    # TBD: Filter out snapshots not managed by this script,
    # e.g. zfs-auto-snap's and/or user-created snaps, to ensure
    # consistency of zoneroot and delegated datasets' comments?
    SNAPNAME="$(zfs list -tsnapshot -Honame -screation -d1 "$ZONEROOT_DS")" \
        && [ -n "$SNAPNAME" ] \
    || { logmsg_error "Got an error listing snapshots of $ZONEROOT_DS"; return 1; }
    echo "$SNAPNAME"
}

get_zone_newest_snapshot_dataset() {
    local ZONENAME="$1"
    local SNAPNAME

    SNAPNAME="$(list_zone_snapshot_datasets "$ZONENAME")" || return
    SNAPNAME="$(echo "$SNAPNAME" | tail -n 1)" \
        && [ -n "$SNAPNAME" ] \
    || { logmsg_error "Got an error determining newest snapshot dataset of $ZONEROOT_DS"; return 1; }
    echo "$SNAPNAME"
}

pipe_get_snapshot_comment() {
    sed 's,^.*@,,'
}

get_zone_newest_snapshot_comment() {
    local ZONENAME="$1"
    local SNAPCOMMENT SNAPNAME
    SNAPNAME="$(get_zone_newest_snapshot_dataset "$ZONENAME")" || return
    SNAPCOMMENT="$(echo "$SNAPNAME" | pipe_get_snapshot_comment)" \
        && [ -n "$SNAPCOMMENT" ] \
    || { logmsg_error "Got an error determining comment of newest snapshot for zone $ZONENAME"; return 1; }
    echo "$SNAPCOMMENT"
}

list_all_zones() {
    zoneadm list -cp | \
        awk -F: '{print $2}' | \
        grep -vw global
}

get_zone_state() {
    local ZONENAME="$1"
    zoneadm list -p -z "${ZONENAME}" | \
        awk -F: '{print $3}'
}

##########################################################################
# Create the golden-image zone from scratch, if the name is not yet occupied
# The zonecfg_snippet_*() routines print a bit of configuration for zonecfg
##########################################################################

# Let specify image template type (linked, brand, etc.) to set up the config
zonecfg_snippet_create() {
    if [ -n "$ORIGIN_GIZ" ] ; then
        echo "create -t $ORIGIN_GIZ"
    else
        if [ -n "$TEMPLATE_GIZ" ] ; then
            echo "create -t $TEMPLATE_GIZ"
        else
            echo "create -b"
            [ -z "$BRAND_GIZ" ] || echo "set brand=$BRAND_GIZ"
        fi
    fi
}

# Minimal config zone-wise (no resources, filesystem/dataset provisions,
# exclusive networking, user accounts...); set autoboot:=false
zonecfg_snippet_basics() {
    [ -n "$TARGET_ZONE" ] || die "TARGET_ZONE is not defined"

    cat << EOF
set ip-type=exclusive
set autoboot=false
EOF

    case "$ACTION" in
        create-giz) echo "set zonepath=$DATASET_CONTAINMENT_GIZ_MPT/$TARGET_ZONE" ;;
        create-diz) echo "set zonepath=$DATASET_CONTAINMENT_DIZ_MPT/$TARGET_ZONE" ;;
        *) logmsg_warn "Uncertain which containment dataset to use for ACTION='$ACTION', so none was set for zone '$TARGET_ZONE'" ;;
    esac
}

# TBD: Let specify package source(s) - URL to IPS repo, possibly just one
# publisher may be allowed for initial installation (depends on illumos
# distro and zone brand - e.g. linked-image zones inherit the whole set of
# publishers from GZ), but more may be added after initial "image" setup.

# TBD: Consider packaging, tarballs, zfs-send images to instantiate zone root

##########################################################################
# Create a golden-image zone from existing golden-image zone (clone current
# state, last or specified snapshot):
##########################################################################

# Add metadata (zonecfg field) to track which GIZ this image was built from
zonecfg_snippet_attr_origin() {
    VAL="undefined"
    if [ -n "$ORIGIN_GIZ" ] ; then
        VAL="giz:$ORIGIN_GIZ"
    else
        if [ -n "$TEMPLATE_GIZ" ] ; then
            VAL="template:$TEMPLATE_GIZ"
        else
            [ -n "$BRAND_GIZ" ] && VAL="brand:$BRAND_GIZ" || VAL="blank"
        fi
    fi

    cat << EOF
add attr
set name=zonemgr.origin
set type=string
set value="$VAL"
end
EOF
    unset VAL
}

# TBD: Specify package(s) to remove from the zone compared to original while
# creating it (e.g. to avoid conflicts for subsequently added packages)

# TBD: Specify package(s) to add into the zone compared to original while
# creating it

# Same limitations (e.g. simplicity of config) as above

##########################################################################
# Create a disposable zone from specified golden-image zone (clone current
# state, last or specified snapshot):
##########################################################################

# Manage VNICs?
create_vnic() {
    # Defines a VNIC on the host using specified LINK and maybe other attrs
    # Input value is "LINK[|VLAN[|MAC]]" string with returned vnic name
    # based on the value of TARGET_ZONE (if present).
    # Output: echoes the vnic name if successfully created
    local LINK VLAN MACADDR VNIC DLADM_ARGS
    LINK="$(echo "$1" | (IFS='|' read L V M ; echo "$L"))"
    if [ -z "$LINK" ]; then
        logmsg_error "Did not find a LINK in specified arguments: '$1'"
        return 1
    fi
    VLAN="$(echo "$1" | (IFS='|' read L V M ; echo "$V"))"
    MACADDR="$(echo "$1" | (IFS='|' read L V M ; echo "$M"))"

    case "$ACTION" in
        create-giz) VNIC="autogiz${TARGET_ZONE}" ;;
        create-diz) VNIC="autodiz${TARGET_ZONE}" ;;
        *) logmsg_warn "Uncertain which VNIC name pattern to use for ACTION='$ACTION'"
           VNIC="auto${TARGET_ZONE}" ;;
    esac
    # VNIC names must be 16 max characters, [a-z0-9_]
    # We leave 3 chars for up to 999 VNIC numbers with a given "basename"
    VNIC="`echo "${VNIC}" | tr 'A-Z' 'a-z' | sed 's,[^a-z0-9_],,g' | cut -b 1-13`"
    VNIC="${VNIC}$(dladm show-vnic | egrep -c "^${VNIC}"'[0-9]*[ \t]')"

    DLADM_ARGS="-l ${LINK}"
    [ -z "$MAC" ] || DLADM_ARGS="$DLADM_ARGS -m ${MAC}"
    [ -z "$VLAN" ] || DLADM_ARGS="$DLADM_ARGS -v ${VLAN}"

    $ELEVATE_CMD dladm create-vnic $DLADM_ARGS "$VNIC" >&2 && \
    echo "$VNIC"
}

delete_vnic() {
    # routine to remove a VNIC definition from host OS (global zone)
    local NICNAME="$1"
    case "$NICNAME" in
        auto?iz*)
            $ELEVATE_CMD dladm delete-vnic "$NICNAME" ;;
        "") logmsg_warn "Requested to remove a VNIC but got no name" ;;
        *)  die "Requested to remove VNIC '$NICNAME' which is not an auto-vnic" ;;
    esac
}

# TBD: Destroy the VNIC when scrapping the zone, but do not use
# temporary VNICs (-t) which only last until reboot... or perhaps
# do so - and require "start_zone()" with support for autovnic, and/or
# TBD: add a way to find and wipe orphaned auto-vnics (not configured
# in any known zone, not necessarily running, and not with a "zoned"
# linkprop) as e.g. a cron job.
list_auto_vnics_all() {
    #$ELEVATE_CMD \
    dladm show-vnic -o LINK | egrep '^auto(giz|diz)'
}

list_auto_vnics_zonecfg() {
    # Returns the chomped value of autovnic attr that is a space-separated
    # list of NICNAME[|LINK[|VLAN[|MAC]]] entries
    local ZONENAME="$1"
    local OUT=""

    OUT="$(zonecfg -z "$ZONENAME" "info attr name=zonemgr.autovnic")" \
    && [ -n "$OUT" ] || return $?

    if echo "$OUT" | grep 'No such attr resource' >/dev/null 2>/dev/null ; then
        # No such attr - vnics never defined?
        return 0
    fi

    # Error here is missing token/unexpected markup
    # Somehow the simple `'s,^[\ \t]*value:[\ \t]*\(.*\)$,\1,'` expression
    # and `'s,^[\ \t]*value:[\ \t]*,,` refused to do the right thing
    OUT="$(echo "$OUT" | grep 'value:' )" \
    && OUT="$(echo "$OUT" | sed -e 's,value:,,' -e 's,^[\ \t]*,,')" \
    && [ -n "$OUT" ] || return $?

    # NOTE: We assume an empty output here a valid one - no autovnics
    OUT="$(echo "$OUT" | sed 's,^\" *\(.*\) *\"$,\1,')" \
    || return $?

    echo "$OUT"
    #TBD # test this on various unsuspecting victims
}

list_delegated_nics_zonecfg() {
    # Returns the an EOL-separated list of NICNAME[ ...] entries (one per
    # line) of any sort of "net" entries delegated into a zone, auto or not.
    local ZONENAME="$1"
    local OUT=""

    OUT="$(zonecfg -z "$ZONENAME" "info net" | grep physical: | sed 's,^.*physical: *,,' | egrep -v '^$')" \
    && [ -n "$OUT" ] || return $?

    echo "$OUT"
}

list_auto_vnics_orphaned() {
    # Print a space-separated list of auto-VNIC names that are not owned
    local AUTOVNICS=( $(list_auto_vnics_all) )
    local AUTOVNICS_OWNED=()
    local ZONENAME VLVM VNIC i

    # Were any autovnics found in host OS?
    [[ "${#AUTOVNICS[*]}" -gt 0 ]] || { echo ""; return 0; }

    # Inspect "claimed" vnics: start with list_auto_vnics_all() and cut off:
    # * "autovnic" linkprops in each known (managed) zone
    for ZONENAME in `list_zones_managed_all` ; do
        for VLVM in list_auto_vnics_zonecfg "$ZONENAME" ; do
            AUTOVNICS_OWNED+=( $(echo "$VLVM" | sed 's,|.*$,,') )
        done
    done

    # * any nics delegated into each known zone
    for ZONENAME in `list_all_zones` ; do
        for VNIC in `list_delegated_nics_zonecfg "$ZONENAME"` ; do
            AUTOVNICS_OWNED+=( $VNIC )
        done
    done

    # * vnics with a "zoned" attribute currently
    AUTOVNICS_OWNED+=( `dladm show-linkprop -p zone -c -o link,value | sed 's,^.*:,,'` )

    # un-list vnics that are owned
    for VNIC in "${AUTOVNICS_OWNED[@]}"; do
        for i in "${!AUTOVNICS[@]}"; do
            if [[ "${AUTOVNICS[i]}" = "${AUTOVNICS_OWNED[0]}" ]]; then
                unset 'AUTOVNICS[i]'
            fi
        done
    done

    echo "${AUTOVNICS[@]}"
}

# TBD: routine to remove VNIC config files from zone contents
# (hostname.INST, dhcp.INST, maybe dladm/nwam/... config edit, /etc/hosts?)
remove_vnic_zonecfg() {
    # routine to remove VNIC snippet from zonecfg
    local ZONENAME="$1"
    local NICNAME="$2"
    local CLEAN_AUTOVNIC="$3"
    check_is_managed_zone "$ZONENAME" || return $?
    [ -n "$NICNAME" ] || return $?

    $ELEVATE_CMD \
    zonecfg -z "$ZONENAME" "remove net physical=$NICNAME"

    if [ "$CLEAN_AUTOVNIC" != "quick" ] ; then
        # Remove from autovnic property too, if asked
        local DELEGATED_VNICS="`list_auto_vnics_zonecfg ${ZONENAME}`"
        if [ -n "$DELEGATED_VNICS" ]; then
            #TBD # Better parsing needed in this chain - not all cases were
                 # caught (e.g. only NICNAME w/o attrs)... and now?
            DELEGATED_VNICS="`echo " $DELEGATED_VNICS " | sed -e 's, '"$NICNAME"'\(\|[^ ]* | \), ,g' -e 's,^[ ]*,,' -e 's, *$,,'`"

# TBD : is "commit" needed for inline calls?
            $ELEVATE_CMD \
            zonecfg -z "$ZONENAME" "remove attr name=zonemgr.autovnic"

            [ -n "$DELEGATED_VNICS" ] && \
            $ELEVATE_CMD zonecfg -z "$ZONENAME" << EOF
add attr
set name=zonemgr.autovnic
set type=string
set value="$VNICS"
end
EOF

        fi
    fi
}

delete_vnics_zone() {
    # Remove ALL of the host (GZ) VNICs previously given to this zone
    # Note the zone should be not-running (VNICs not busy) at this time
    local ZONENAME="$1"
    check_is_managed_zone "$ZONENAME" || return $?
    local DELEGATED_VNICS="`list_auto_vnics_zonecfg ${ZONENAME} | sed 's,|[^ ]*,,g'`"
    local V

    [ -n "$DELEGATED_VNICS" ] && \
    logmsg_info "Removing auto-VNICs from host: $DELEGATED_VNICS" && \
    for V in $DELEGATED_VNICS ; do
        delete_vnic "$V"
    done
}

remove_vnic_zonecfg_all() {
    # routine to remove VNIC snippet from zonecfg
    local ZONENAME="$1"
    check_is_managed_zone "$ZONENAME" || return $?
    local DELEGATED_VNICS="`list_auto_vnics_zonecfg ${ZONENAME} | sed 's,|[^ ]*,,g'`"
    local V

    [ -n "$DELEGATED_VNICS" ] && \
    logmsg_info "Removing auto-VNICs from zone '$ZONENAME' config: $DELEGATED_VNICS" && \
    for V in $DELEGATED_VNICS ; do
        remove_vnic_zonecfg "$Z" "$V" "quick"
    done

    $ELEVATE_CMD \
    zonecfg -z "$ZONENAME" "remove attr name=zonemgr.autovnic"
}

# Add resource definitions (datasets/lofs, network, user accounts/ldap, ...)
# TBD: Constrain resources (cap memory/swap, set cpu-shares, ncpus and/or
# host-defined resource pools in zonecfg; set tmpfs limits in vfstab)
zonecfg_snippet_assign_nic() {
    local NICNAME="$1"
    cat << EOF
add net
set physical=$NICNAME
end
EOF
    # TBD: Do we need a "match" for the device too? Seems not required on OI
    # TBD: Do we need to precreate dev/x -> dev/net/x symlinks in zone
    # container of the rootfs? Probably not for builders, maybe yes for
    # infra zones like firewalls/routers based on a common dummy...
}

zonecfg_snippet_assign_use_vnics() {
    # Technically the existing interface may also be a normal dedicated NIC
    # but that is such a rare case nowadays, so this routine and vars are
    # named for VNICs to match similar other code.
    local VNIC

    for VNIC in $USE_VNICS ; do
        zonecfg_snippet_assign_nic "$VNIC"
    done
}

zonecfg_snippet_assign_auto_vnics() {
    local VNICS
    local LVM_LIST="$*"
    if [ $# = 0 ] || [ -z "$LVM_LIST" ]; then
        LVM_LIST="$AUTO_VNICS"
    fi

    # Find all previously defined VNICs (if any) to append to their list.
    # If any are present, wipe the list before we'd "add" the attribute.
    # Note this routine returns the attr in format we append to below -
    # no cleaning is needed. This format allows to re-create the VNICs
    # before zone startup if they are currently missing (e.g. temporary
    # and the host rebooted). This is not in scope of standard zone startup.
    # Also note that with temporary VNICs it is possible that the name we
    # have saved, gets occupied by another zone's autovnic. This collides
    # with hypothetical ability to reuse an already defined VNIC by another
    # zone. Either case can cause inability of the zone to configure or
    # start up, and (TBD?) should be figured out by sysadmin who brought
    # the mess on themselves. Alternatively (TBD), for a temporary autovnic
    # we can discard the device name (at least if it is not available) and
    # just generate/instantiate another to the same LVM spec... Finally
    # note that the VNIC device name can be referenced in the zone itself
    # (/etc/hostname.$VNIC, its NWAM, networking daemon setups, etc.) or
    # in external (GZ-based) software setups. So handling the renaming or
    # temporary VNICs (with changing names) can have arbitrary complexity.
    VNICS="$(list_auto_vnics_zonecfg "${TARGET_ZONE}")" 2>/dev/null \
    && [ -n "$VNICS" ] && echo "remove attr name=zonemgr.autovnic" \
    || VNICS=""

    for LVM in $LVM_LIST ; do
        NEWVNIC="$(create_vnic "$LVM")" && [ -n "$NEWVNIC" ] \
            || die "Failed to create VNIC by spec: $LVM"
        [ -z "$VNICS" ] && VNICS="$NEWVNIC|$LVM" || VNICS="$VNICS $NEWVNIC|$LVM"
        zonecfg_snippet_assign_nic "$NEWVNIC"
    done

    if [ -n "$VNICS" ]; then
         cat << EOF
add attr
set name=zonemgr.autovnic
set type=string
set value="$VNICS"
end
EOF
    fi
}

zonecfg_snippet_assign_dataset() {
    local DSNAME="$1"
    cat << EOF
add dataset
set name=$DSNAME
end
EOF
}

zonecfg_snippet_unassign_dataset() {
    # Just stop listing the delegated dataset in this zone config
    # This does not remove or change the ZFS dataset itself
    local DSNAME="$1"
    echo "remove dataset name=$DSNAME"
}

zonecfg_snippet_assign_dataset_list() {
    # $1 == ADD_DELEGATED_DATASET, a whitespace-separated list
    if [ -n "$1" ]; then
        local DSNAME
        for DSNAME in $1 ; do
	    zonecfg_snippet_assign_dataset "$DSNAME"
        done
    fi
}

zonecfg_snippet_unassign_dataset_list() {
    # $1 == ADD_DELEGATED_DATASET, a whitespace-separated list
    if [ -n "$1" ]; then
        local DSNAME
        for DSNAME in $1 ; do
	    zonecfg_snippet_unassign_dataset "$DSNAME"
        done
    fi
}

list_assigned_datasets() {
    local N D
    local ZONENAME="$1"

    LANG=C LC_ALL=C \
    zonecfg -z "$ZONENAME" "info dataset" | while read N D ; do
        [ "$N" = "name:" ] && echo "$D"
    done
}

secure_zoned_dataset() {
    local DSNAME="$1"

    zfs_create_recursive "$DSNAME" || return
    if is_mounted "$DSNAME" ; then
        $ELEVATE_CMD zfs umount "$DSNAME" || \
            $ELEVATE_CMD umount "$DSNAME"
    fi
    $ELEVATE_CMD zfs set canmount=off "$DSNAME"
    $ELEVATE_CMD zfs set mountpoint=legacy "$DSNAME"
    $ELEVATE_CMD zfs set zoned=on "$DSNAME"
}

secure_assigned_datasets() {
    local DS
    local RES=0
    local ZONENAME="$1"
    for DS in `list_assigned_datasets "$ZONENAME"` ; do
        secure_zoned_dataset "$DS" || RES=$?
    done
    return $RES
}

zonecfg_snippet_assign_lofs() {
    local ZONEDIR="$1"
    local HOSTDIR="$2"
    [ -n "${HOSTDIR-}" ] || HOSTDIR="${ZONEDIR-}"
    [ -n "${ZONEDIR-}" ] || die "Bad args to zonecfg_snippet_assign_lofs()"
    cat << EOF
add fs
set dir=$ZONEDIR
set special=$HOSTDIR
set type=lofs
add options [rw,nodevices]
end
EOF
}

zonecfg_snippet_assign_lofs_list() {
    # $1 = $ADD_MOUNT_LOFS, list of "lzpath[|gzpath] lzpath2[|gzpath2] ..."
    local LZPATH GZPATH L_G
    if [ -n "$1" ]; then
        for L_G in $1 ; do
            LZPATH="`echo "$L_G" | ( IFS='|' read _1 _2 ; echo "${_1}" )`"
            GZPATH="`echo "$L_G" | ( IFS='|' read _1 _2 ; echo "${_2}" )`"
            zonecfg_snippet_assign_lofs "$LZPATH" "$GZPATH"
        done
    fi
}

zonecfg_snippet_assign_autonfs() {
    TBD
}

zonecfg_snippet_assign_autonfs_list() {
    TBD # Decide how to implement autonfs better, including installation
    # and enablement of the autofs and nfs/client services.
    # Oh, and configs. And /etc/hosts or DNS or ldap naming, perhaps...

    # $1 = $ADD_MOUNT_AUTONFS, list of "lzpath|NFSurl lzpath2|NFSurl2 ..."
    local LZPATH NFSURL L_N
    if [ -n "$1" ]; then
        for L_N in $1 ; do
            LZPATH="`echo "$L_N" | ( IFS='|' read _1 _2 ; echo "${_1}" )`"
            NFSURL="`echo "$L_N" | ( IFS='|' read _1 _2 ; echo "${_2}" )`"
            zonecfg_snippet_assign_autonfs "$LZPATH" "$NFSURL"
        done
    fi
}

# Scripted cloning of file-based user/shadow and group account data from GZ
# to LZ for respective specified accounts. Note that (stub for) LDAP is below.
# TBD: Set up root password, at least for initial GIZ
# TBD: support togglig root as user/role?
# TBD: maybe support other sysidcfg or equivalent?
clone_group() {
    # TODO: By construction, this routine now requires "root" to run it.
    # If this proves a bottleneck, find a way to elevate privs to do the job.
    # Perhaps mount/ready the zone, "$ELEVATE_CMD zlogin -S" to fix it up...
    local G="$1"
    local _G

    _G="$(egrep "^$G:" "/etc/group")" || \
        die "Can not replicate unknown group '$G' from the host!"

    if [ "$MY_UID" = 0 ] ; then
        getALTROOT && \
        [ -d "$ALTROOT/etc" ] || die "Not a directory: '$ALTROOT/etc'"

        logmsg_info "Defining group account '$_G' from host to zone as GZ root"
        if egrep "^$G:" "${ALTROOT}/etc/group" >/dev/null ; then
            egrep -v "^$G:" < "${ALTROOT}/etc/group" > "${ALTROOT}/etc/group.tmp" && \
            echo "$_G" >> "${ALTROOT}/etc/group.tmp" && \
            cat "${ALTROOT}/etc/group.tmp" > "${ALTROOT}/etc/group"
            rm -f "${ALTROOT}/etc/group.tmp"
        else
            echo "$_G" >> "${ALTROOT}/etc/group"
        fi
    else
        logmsg_info "Defining group account '$_G' from host to zone via zlogin"
        $ELEVATE_CMD zlogin -S "${TARGET_ZONE}" << EOF
if egrep "^$G:" "/etc/group" >/dev/null ; then
    egrep -v "^$G:" < "/etc/group" > "/etc/group.tmp" && \
    echo "$_G" >> "/etc/group.tmp" && \
    cat "/etc/group.tmp" > "/etc/group"
    rm -f "/etc/group.tmp"
else
    echo "$_G" >> "/etc/group"
fi
EOF
    fi
}

clone_user() {
    # TODO: By construction, this routine now requires "root" to run it.
    # If this proves a bottleneck, find a way to elevate privs to do the job.
    local U="$1"
    local _P=""
    local _S=""

    _P="$($ELEVATE_CMD egrep "^$U:" "/etc/passwd")" || \
        die "Can not replicate unknown user '$U' from the host!"

    if [ "$MY_UID" = 0 ] ; then
        _S="$(egrep "^$U:" "/etc/shadow")" || _S=""
    else
        # TODO: This magic also has CPIO headers in the stream, effectively
        # adding cruft in the first and last lines
        _S="$(echo /etc/shadow | $ELEVATE_CMD cpio -ocq | tr '\0' '\n' | egrep "^$U:")" || _S=""
    fi

    if [ -z "$_S" ] ; then
        _S="$U:NP:::::::"
        logmsg_warn "Can not read password hash of user account '$U' on the host," \
            "using an initial NP account setting in the zone!"
    fi

    if [ "$MY_UID" = 0 ] ; then
        getALTROOT && \
        [ -d "$ALTROOT/etc" ] || die "Not a directory: '$ALTROOT/etc'"

        logmsg_info "Defining user account '$_P' from host to zone as GZ root"
        if egrep "^$U:" "${ALTROOT}/etc/passwd" >/dev/null ; then
            egrep -v "^$U:" < "${ALTROOT}/etc/passwd" > "${ALTROOT}/etc/passwd.tmp" && \
            echo "$_P" >> "${ALTROOT}/etc/passwd.tmp" && \
            cat "${ALTROOT}/etc/passwd.tmp" > "${ALTROOT}/etc/passwd"
            rm -f "${ALTROOT}/etc/passwd.tmp"
        else
            echo "$_P" >> "${ALTROOT}/etc/passwd"
        fi

        if egrep "^$U:" "${ALTROOT}/etc/shadow" >/dev/null ; then
            egrep -v "^$U:" < "${ALTROOT}/etc/shadow" > "${ALTROOT}/etc/shadow.tmp" && \
            echo "$_S" >> "${ALTROOT}/etc/shadow.tmp" && \
            cat "${ALTROOT}/etc/shadow.tmp" > "${ALTROOT}/etc/shadow"
            rm -f "${ALTROOT}/etc/shadow.tmp"
        else
            echo "$_S" >> "${ALTROOT}/etc/shadow"
        fi
    else
        logmsg_info "Defining user account '$_P' from host to zone via zlogin"
        $ELEVATE_CMD zlogin -S "${TARGET_ZONE}" << EOF
if egrep "^$U:" "/etc/passwd" >/dev/null ; then
    egrep -v "^$U:" < "/etc/passwd" > "/etc/passwd.tmp" && \
    echo "$_P" >> "/etc/passwd.tmp" && \
    cat "/etc/passwd.tmp" > "/etc/passwd"
    rm -f "/etc/passwd.tmp"
else
    echo "$_P" >> "/etc/passwd"
fi

if egrep "^$U:" "/etc/shadow" >/dev/null ; then
    egrep -v "^$U:" < "/etc/shadow" > "/etc/shadow.tmp" && \
    echo "$_S" >> "/etc/shadow.tmp" && \
    cat "/etc/shadow.tmp" > "/etc/shadow"
    rm -f "/etc/shadow.tmp"
else
    echo "$_S" >> "/etc/shadow"
fi
EOF
    fi
}

clone_groups() {
    local RES=0
    [ -z "${COPY_GROUPS-}" ] || \
    for G in $COPY_GROUPS ; do
        clone_group "$G" || RES=$?
    done
    return $RES
}

clone_users() {
    local RES=0
    [ -z "${COPY_USERS-}" ] || \
    for U in $COPY_USERS ; do
        clone_user "$U" || RES=$?
    done
    return $RES
}

elevate_user() {
    local U="$1"

    if [ "$MY_UID" = 0 ] ; then
        getALTROOT && \
        [ -d "$ALTROOT/etc" ] || die "Not a directory: '$ALTROOT/etc'"

        logmsg_info "Elevating user account '$U' in the zone as GZ root"
        mkdir -p "${ALTROOT}/etc/sudoers.d/" "${ALTROOT}/etc/user_attr.d/"
        printf '%s\tALL=(ALL) NOPASSWD: ALL\n' "$U" > "${ALTROOT}/etc/sudoers.d/90zonemgr.$U"
        chmod 600 "${ALTROOT}/etc/sudoers.d/90zonemgr.$U"
        chown 0:0 "${ALTROOT}/etc/sudoers.d/90zonemgr.$U"
        printf '%s::::roles=root;min_label=admin_low;lock_after_retries=no;auths=solaris.*,solaris.grant;audit_flags=lo\\:no;profiles=All;clearance=admin_high;type=normal\n' "$U" > "${ALTROOT}/etc/user_attr.d/90zonemgr.$U"
    else
        logmsg_info "Elevating user account '$U' in the zone via zlogin"
        $ELEVATE_CMD zlogin -S "${TARGET_ZONE}" << EOF
mkdir -p "/etc/sudoers.d/" "/etc/user_attr.d/"
printf '%s\tALL=(ALL) NOPASSWD: ALL\n' "$U" > "/etc/sudoers.d/90zonemgr.$U"
chmod 600 "/etc/sudoers.d/90zonemgr.$U"
chown 0:0 "/etc/sudoers.d/90zonemgr.$U"
printf '%s::::roles=root;min_label=admin_low;lock_after_retries=no;auths=solaris.*,solaris.grant;audit_flags=lo\\:no;profiles=All;clearance=admin_high;type=normal\n' "$U" > "/etc/user_attr.d/90zonemgr.$U"
EOF
    fi
}

elevate_users() {
    local RES=0
    [ -z "${ELEVATE_USERS-}" ] || \
    for U in $ELEVATE_USERS ; do
        elevate_user "$U" || RES=$?
    done
    return $RES
}

# TBD: Configure LDAP (hosts, users, groups, netgroups; client; registration;
# pam; an openldap.conf for other tools) by editing and/or adding files in NGZ
setup_ldap() {
    TBD
}

setup_vnic_dhcp() {
    # Pre-set NICs for legacy DHCP (/etc/dhcp.VNICNAME), if not already set up.
    # Assumes zone root is mounted, and "ready" or "booted".
    local ZONENAME="$1"
    local NICNAME="$2"

    # TODO: forcing-flag to not skip existing setups?
    $ELEVATE_CMD zlogin -S "${ZONENAME}" << EOF
if [ -s "/etc/hostname.${NICNAME}" ] ; then
    echo "SKIP: NIC '${NICNAME}' is already set up in zone '$ZONENAME'"
else
    cat /dev/null > /etc/dhcp.${NICNAME}
    cat /dev/null > /etc/hostname.${NICNAME}
    rm -f /etc/dhcp/${NICNAME}.dhc
fi
EOF
}

setup_zone_vnics_dhcp() {
    local ZONENAME="$1"
    local NICNAME DELEGATED_NICS EXISTING_NICS

    [ -n "$ZONENAME" ] || ZONENAME="${TARGET_ZONE}"

    DELEGATED_NICS="$(list_delegated_nics_zonecfg "$ZONENAME")" \
    && [ -n "$DELEGATED_NICS" ] \
    || { logmsg_info "No NICs found delegated into zone '$ZONENAME', nothing to configure here"
         return 0 ; }
    # TODO: Clean up existing configs of NICs that are not delegated,
    # in case of untidy origin zone being cloned...

    for NICNAME in $DELEGATED_NICS ; do
        setup_vnic_dhcp "$ZONENAME" "$NICNAME"
    done

    # Note: when the zone is in single-user mode, we can not manipulate the
    # services normally because of "svcadm: Couldn't bind to configuration
    # repository: repository server unavailable."
    $ELEVATE_CMD zlogin -S "${ZONENAME}" << EOF
svcadm disable -s svc:/network/physical:nwam || true
svcadm enable svc:/network/physical:default
EOF
}

setup_zone_name() {
    local ZONENAME="$1"
    local ZONEHOSTNAME="$2"
    [ -n "$ZONENAME" ] || ZONENAME="${TARGET_ZONE}"
    [ -n "$ZONEHOSTNAME" ] || ZONEHOSTNAME="${ZONENAME}"

    $ELEVATE_CMD zlogin -S "${ZONENAME}" << EOF
echo "${ZONEHOSTNAME}" > "/etc/nodename"
if [ -n "${ORIGIN_ZONE}" ] ; then
    sed -e 's,^\(127\.0\.0\.1.*[\ \t]\)'"${ORIGIN_ZONE}"'\([\ \t].*|\)$,\1\2,' \
        -i /etc/hosts
fi
    sed -e 's,^\(127\.0\.0\.1.*[\ \t].*\)$,\1\t'"${ZONEHOSTNAME}"',' \
        -i /etc/hosts
if [ x"${ZONEHOSTNAME}" != x"${ZONENAME}" ]; then
    sed -e 's,^\(127\.0\.0\.1.*[\ \t].*\)$,\1\t'"${ZONENAME}"',' \
        -i /etc/hosts
fi
EOF
}

##########################################################################
# General maintenance actions:
##########################################################################

# List all managed, or only GIZ/only DIZ zones (zones with roots under our
# container datasets)
#   Assume no whitespaces in path and zone names
#   Support legacy dumber zoneadm syntax
do_list_zones_managed_giz() {
    # TBD: combine grep and awk when I'd be in illumos (zonepath is $4 IIRC?)
    # and verify zonename is the $2
    zoneadm list -cp | grep ":$DATASET_CONTAINMENT_GIZ_MPT/" | \
        awk -F: '{print $2}'
}

do_list_zones_managed_diz() {
    zoneadm list -cp | grep ":$DATASET_CONTAINMENT_DIZ_MPT/" | \
        awk -F: '{print $2}'
}

do_list_zones_managed_all() {
    zoneadm list -cp | \
        egrep ":($DATASET_CONTAINMENT_GIZ_MPT|$DATASET_CONTAINMENT_DIZ_MPT)/" | \
        awk -F: '{print $2}'
}

zonelist_args() {
    [ $# != 0 ] || return 1
    local REGEX="`join_regex "$@"`" || return

    case "$LIST_ARGS" in
	*p*) # No header, machine-parseable
            zoneadm list $LIST_ARGS | egrep ":$REGEX:" ;;
	*v*) # One header line, human-readable
            zoneadm list $LIST_ARGS | awk '($2 ~ /^'"$REGEX"'$|^NAME$/) {print $0}' ;;
        *i*|*c*) # Just list of names of zones in certain status
            zoneadm list $LIST_ARGS | egrep '^'"$REGEX"'$' ;;
    esac
}

list_zones_managed_giz() {
    if [ -z "$LIST_ARGS" ] ; then
        do_list_zones_managed_giz
    else
        zonelist_args `do_list_zones_managed_giz`
    fi
}

list_zones_managed_diz() {
    if [ -z "$LIST_ARGS" ] ; then
        do_list_zones_managed_diz
    else
        zonelist_args `do_list_zones_managed_diz`
    fi
}

list_zones_managed_all() {
    if [ -z "$LIST_ARGS" ] ; then
        do_list_zones_managed_all
    else
        zonelist_args `do_list_zones_managed_all`
    fi
}

get_zone_root() {
    local ZONENAME="$1"
    [ -n "$ZONENAME" ] && \
    zoneadm -z "$ZONENAME" list -p 2>/dev/null | \
        awk -F: '{print $4}'
}

get_zone_root_ds() {
    local ZONENAME="$1"
    zfs_getDSbyMPT "`get_zone_root "$ZONENAME"`"
}

get_zone_state() {
    local ZONENAME="$1"
    [ -n "$ZONENAME" ] && \
    zoneadm -z "$ZONENAME" list -p 2>/dev/null | \
        awk -F: '{print $3}'
}

is_managed_zone() {
    # Returns TRUE if the zonepath of "$1" is under GIZ or DIZ container
    local ZONENAME="$1"
    get_zone_root "$ZONENAME" | \
        egrep "^($DATASET_CONTAINMENT_GIZ_MPT|$DATASET_CONTAINMENT_DIZ_MPT)/" >/dev/null
}

check_is_managed_zone() {
    local ZONENAME="$1"
    [ -n "$ZONENAME" ] || \
        { logmsg_error "No zone specified, while asked to inspect it" ; return 1; }
    if ! is_managed_zone "$@" ; then
        logmsg_error "Can not confirm that zone '$*' can be managed by this script"
        return 1
    fi
}

is_managed_zone_diz() {
    local ZONENAME="$1"
    get_zone_root "$ZONENAME" | \
        egrep "^($DATASET_CONTAINMENT_DIZ_MPT)/" >/dev/null
}

is_managed_zone_giz() {
    local ZONENAME="$1"
    get_zone_root "$ZONENAME" | \
        egrep "^($DATASET_CONTAINMENT_GIZ_MPT)/" >/dev/null
}

# Iterate all golden-image zones to upgrade them (e.g. from crontab)
# We can upgrade other zones (e.g. DIZ) but on a case-by-case basis
# (e.g. an explicit request to test the updateability), not wholesale
upgrade_zones_giz() {
    for Z in $(list_zones_managed_giz) ; do
        upgrade_zone "$Z"
    done
}

##########################################################################
# General actions against any managed zones (GIZ or DIZ):
##########################################################################

# Generate a unique name-part for the zone (hash of concat of sorted requested
# pkg names?) that may get prefixed and/or suffixed for GIZ and DIZ instances
# In particular, this should help compare which zone to use as origin (and/or
# to create an intermediate GIZ to clone more such DIZes).
# TBD: Store a plaintext list of required packages in a zonecfg attr?
generate_zonename() {
    # TBD : See comment above about hashing the zone name with useful content
    logmsg_trace "Generating a zone name..." >&2

    local EXISTING_ZONES="`zoneadm list -p | awk -F: '{print $2}'`"
    local TRYNAME
    local Z

    # NOTE: Infinite loop possible here, if all 99999 random numbers are
    # already used for this PID number. I guess the server boils sooner.
    # Limits are 64 chars, not "global" and not starting with "SUNW"...
    while : ; do
        TRYNAME="autozone-$$-${RANDOM}"
        [ -z "$EXISTING_ZONES" ] && break
        for Z in $EXISTING_ZONES ; do
            [ "$Z" = "${TRYNAME}" ] && continue 2
        done
        break
    done

    logmsg_trace "Generated a zone name: $TRYNAME" >&2
    if [ -n "$OUTFILE" ]; then
        echo "GENERATED_ZONENAME='$TRYNAME'" >> "$OUTFILE"
    fi
    echo "$TRYNAME"
}

generate_zonecfg_snippets() {
    logmsg_trace "Generating zonecfg snippets..." >&2

    zonecfg_snippet_create
    zonecfg_snippet_basics
    zonecfg_snippet_attr_origin
    # TBD: Should a GIZ have no auto-vnics? Or can it have some,
    # just like datasets for DIZ's to replicate too when cloning?
    #[ "$ACTION" = create-diz ] && \
    zonecfg_snippet_assign_auto_vnics
    zonecfg_snippet_assign_use_vnics

    zonecfg_snippet_assign_dataset_list "$ADD_DELEGATED_DATASET"
    zonecfg_snippet_assign_lofs_list "$ADD_MOUNT_LOFS"
    # TBD # zonecfg_snippet_assign_autonfs_list "$ADD_MOUNT_AUTONFS"
    echo "verify"
    echo "commit"

    logmsg_trace "Generated zonecfg snippets" >&2
}

generate_zonecfg_script() {
    logmsg_trace "Generating zonecfg script..." >&2
    ZONECFG_SCRIPT="$(generate_zonecfg_snippets)" && [ -n "$ZONECFG_SCRIPT" ] \
        || die "Could not generate ZONECFG_SCRIPT"
    logmsg_trace "Generated zonecfg script" >&2
}

# Snapshot a zone
snapshot_zone() {
    local ZONENAME="$1"
    local SNAPCOMMENT="$2"
    if [ -z "$SNAPCOMMENT" ] ; then
        logmsg_info "No snapshot comment specified for the zone, using timestamp or PID"
        SNAPCOMMENT="autozone-`timestamp_now`" || \
            SNAPCOMMENT="autozone-$$"
    fi

    logmsg_info "Snapshotting zone '${ZONENAME}'@'${SNAPCOMMENT}'"
    check_is_managed_zone "$ZONENAME" || return

    local WAS_MOUNTED
    local WAS_RUNNING="`get_zone_state "$ZONENAME"`"

    # Stop zone? (option? - note zone for cloning must be stopped per docs)
    if [ "$WAS_RUNNING" = running ]; then
        stop_zone "${ZONENAME}"
    fi

    # Export zonecfg and/or XML into zoneroot container dataset?
    local ZONEROOT_MPT="`get_zone_root "${ZONENAME}"`" || return
    local ZONEROOT_DS="`zfs_getDSbyMPT "${ZONEROOT_MPT}"`" || return

    # If the zoneroot was not mounted, mount it.
    # When it is mounted, export zoneconfig there (and unmount back if needed).
    if is_mounted "$ZONEROOT_MPT" ; then
        WAS_MOUNTED=yes
    else
        $ELEVATE_CMD zfs mount "$ZONEROOT_MPT" || \
        $ELEVATE_CMD zfs mount "$ZONEROOT_DS"
        WAS_MOUNTED=no
    fi
    if is_mounted "$ZONEROOT_MPT" ; then
        # Create an XML file equivalent to /etc/zones/$ZONENAME.xml
        # NOTE : this operation requires root access to place the file into
        # zone container directory. For non-root users there are hoops to jump
        # through. The "Media Restore" privilege is required to "pfexec cpio".
        logmsg_info "Exporting zone config"

        local ZONECFG=""
        local ZONECFG_RES=0
        ZONECFG="$($ELEVATE_CMD /bin/sh -c "zoneadm -z '$ZONENAME' detach -n")" && \
        [ -n "$ZONECFG" ] || ZONECFG_RES=$?

        [ "$ZONECFG_RES" = 0 ] && \
        if [ "$MY_UID" = 0 ] ; then
            echo "$ZONECFG" > "$ZONEROOT_MPT/$ZONENAME.xml"
        else
            local ZONECFG_TMPDIR
            ZONECFG_TMPDIR="$(mktemp -d)" && \
            [ -n "$ZONECFG_TMPDIR" ] || ZONECFG_RES=$?

            [ "$ZONECFG_RES" = 0 ] && \
            echo "$ZONECFG" > "$ZONECFG_TMPDIR/$ZONENAME.xml" && \
            ( cd "$ZONECFG_TMPDIR" && echo "$ZONENAME.xml" \
              | $ELEVATE_CMD /usr/bin/cpio -pmuq "$ZONEROOT_MPT/" ) \
            || ZONECFG_RES=$?

            [ -n "$ZONECFG_TMPDIR" ] && [ -d "$ZONECFG_TMPDIR" ] \
            && rm -f "$ZONECFG_TMPDIR/$ZONENAME.xml" \
            && rm -rf "$ZONECFG_TMPDIR"
        fi

        if [ "$ZONECFG_RES" != 0 ] ; then
            logmsg_error "Could not save zone configuration of '$ZONENAME', going on without it"
        fi
    fi
    if [ "$WAS_MOUNTED" = no ]; then
        $ELEVATE_CMD zfs umount "$ZONEROOT_MPT"
    fi

    # Snapshot zoneroot container dataset + delegated datasets
    local D SNAP_RES
    SNAP_RES=0
    for D in "$ZONEROOT_DS" `list_assigned_datasets ${ZONENAME}` ; do
        zfs list "$D@$SNAPCOMMENT" > /dev/null 2>&1 || \
        $ELEVATE_CMD zfs snapshot -r "$D@$SNAPCOMMENT" || SNAP_RES=$?
    done

    # Restart zone? (if was stopped by us)
    if [ "$WAS_RUNNING" = running ]; then
        start_zone "${ZONENAME}"
    fi
    return $SNAP_RES
}

verify_zone() {
    local ZONENAME="${1-}"
    [ -n "$ZONENAME" ] || \
        { logmsg_error "No zone specified, while asked to inspect it" ; return 1; }
    $ELEVATE_CMD zoneadm -z "$ZONENAME" verify || \
        { clean_exit "Could not verify configuration of zone '$ZONENAME'" ; return 1; }
    check_is_managed_zone "$ZONENAME"
}

install_zone() {
    if [ -z "$TARGET_ZONE" ] ; then
        TARGET_ZONE="$(generate_zonename)"
        logmsg_warn "A TARGET_ZONE name was not provided, generated '$TARGET_ZONE'"
    fi
    logmsg_info "Verifying that zone '${TARGET_ZONE}' is not yet configured..."
    if check_is_managed_zone "$TARGET_ZONE" ; then
        die "Could not configure zone '$TARGET_ZONE': it already exists, destroy it first if needed"
        # TBD: forcing or re-creation is not supported by the script... should it be?
    fi

    generate_zonecfg_script

    logmsg_info "Configuring zone '${TARGET_ZONE}' with the following zonecfg script:" \
        "
=======
${ZONECFG_SCRIPT}
=======
"

    echo "$ZONECFG_SCRIPT" | $ELEVATE_CMD zonecfg -z "${TARGET_ZONE}" \
        || die "Could not configure zone '$TARGET_ZONE' with this script:
=======
$ZONECFG_SCRIPT
======="
    verify_zone "${TARGET_ZONE}"
    secure_assigned_datasets "${TARGET_ZONE}"

    # TBD: Older Solarises - precreate zoneroot dataset?
    # TBD: Use args to template+brand based install to specify publishers
    # and packages, or just install + later revise?
    # PLAN: If publisher list exists, preset the first pub as the zone's
    # install one; then after initial minimal install add other pub's and
    # add/remove packages - using pkg -R from GZ.
    # TODO: Discover how the host GZ uses multiple publishers (its own)
    # by default, when running "install" without the publisher option?
    logmsg_info "Installing zone '${TARGET_ZONE}' (note: at this time special options like publisher and pkg lists are not handled yet)"
    $ELEVATE_CMD zoneadm -z "${TARGET_ZONE}" install \
        || die "Could not install zone '$TARGET_ZONE'"
    snapshot_zone "${TARGET_ZONE}" "initial-install"

    # Add pkgs/pubs?
    logmsg_info "Post-Install processing for zone '${TARGET_ZONE}'"
    setup_zone_postinstall
    snapshot_zone "${TARGET_ZONE}" "initial-postinstalled"
}

# Clone a zone (clone current state, last or specified snapshot)
# This may become a better tuned and flexible solution than "zoneadm clone"
clone_zone() {
    # If TARGET_ZONE is not set, generate one
    if [ -z "$TARGET_ZONE" ] ; then
        TARGET_ZONE="$(generate_zonename)"
        logmsg_warn "A TARGET_ZONE name was not provided, generated '$TARGET_ZONE'"
    fi
    if check_is_managed_zone "$TARGET_ZONE" ; then
        die "Could not configure zone '$TARGET_ZONE': it already exists, destroy it first if needed"
        # TBD: forcing or re-creation is not supported by the script... should it be?
    fi

    # If ORIGIN_ZONE is not set or is missing, install it first and
    # then clone it here - so we can later quickly spawn same setups
    if [ -z "$ORIGIN_ZONE" ]; then
        ORIGIN_ZONE="$(generate_zonename)"
        logmsg_warn "An ORIGIN_ZONE name was not provided, generated '$ORIGIN_ZONE'"
    fi

    if ! check_is_managed_zone "$ORIGIN_ZONE" ; then
        # TBD: Here we check if a zone by this name is not installed at all
        # and do not look at the type (GIZ/DIZ). Not sure yet if this is a
        # desirable behavior (should we let clone DIZ if asked explicitly)?
        logmsg_info "An ORIGIN_ZONE='$ORIGIN_ZONE' was not installed," \
            "preparing it as a GIZ to clone later into TARGET_ZONE='$TARGET_ZONE' DIZ"
        ( TARGET_ZONE="$ORIGIN_ZONE" ; install_zone ) \
            || die "FAILED to install an ORIGIN_ZONE='$ORIGIN_ZONE'"
    fi

    TBD
    # If using existing snapshot, use the stashed copy of that older
    # zone config as template (if differs from current) - create
    # a temporary zonecfg and overwrite with stashed file, etc.,
    # otherwise make snapshot of origin and use current zone config.
    # Clone the zoneroot, warn if any delegated datasets are overboard
    # TODO: clone delegated datasets (figure out where to root the new tree,
    #       and maybe play around the zoned=on and automount properties.
    # Carry over local/received dataset properties to the clones.
    # Generate new zone config via snippets
    # If the new zone config has delegated datasets, make sure they exist
    # and have proper rights and zfs attrs
    #   secure_assigned_datasets "${TARGET_ZONE}"
    # TBD : verify that origin NICs (if any - should not be in a golden image
    # in the first place) are not used in new zone
    setup_zone_postinstall
    snapshot_zone "${TARGET_ZONE}" "clone-postinstalled"
}

setup_zone_postinstall() {
    singleuser_zone "${TARGET_ZONE}"

    setup_zone_name
    clone_groups
    clone_users
    elevate_users
    # setup_ldap

    # If VNICs exist:
    # * Pre-set VNICs for legacy DHCP (/etc/dhcp.VNICNAME).
    # * Enable network/physical:default
    # * Enable ipfilter? Pre-set for config via /etc/ipf/* files?
    # * TODO: Add a CLI option for more flexible config, e.g. static?
    setup_zone_vnics_dhcp

    # TODO: Add a CLI option for custom post-install script sourced from
    # this one into a subshell (inheriting the variables and functions),
    # so end-users can copy more files, etc. Running still in GZ context,
    # thus chrooting/zlogin'ing is up to that scripter.

    halt_zone "${TARGET_ZONE}"
}

# pkg-update contents of a (golden-image?) zone, snapshot after success
upgrade_zone() {
    local ZONENAME="$1"
    TBD
    # snapshot zoneroot+delegations before update
    # zoneadm mount/ready the zone (if not running)
    # pkg update in the zoneroot from GZ
    # zoneadm halt the zone (if was not running initially)
    # snapshot zoneroot+delegations if successful
}

# Configure specified publisher(s) into the named zone, snapshot after success
install_publishers() {
    TBD
}

# Install specified package(s) into the named zone, snapshot after success
install_packages() {
    TBD
}

# Start, stop a zone
zone_action() {
    if [ "$#" = 2 ] ; then
        local ZONEACTION="$1"
        local ZONENAME="$2"

        [ -n "$ZONENAME" ] || die "No zone specified, while required for action '$ZONEACTION'"
        check_is_managed_zone "$ZONENAME" || return
        logmsg_info "Trying to '$ZONEACTION' zone '$ZONENAME'..."
        case "$ZONEACTION" in
            boot|halt|reboot|ready|mount|unmount|boot-single)
                case "$ZONEACTION" in
                    boot|boot-single|reboot|ready|mount)
                        secure_assigned_datasets "$ZONENAME" ;;
                esac
                case "$ZONEACTION" in
                    boot-single)
                        $ELEVATE_CMD \
                            zoneadm -z "$ZONENAME" boot -s
                        return $?
                        ;;
                    *)
                        $ELEVATE_CMD \
                            zoneadm -z "$ZONENAME" "$ZONEACTION"
                        return $?
                        ;;
                esac
                ;;
            poweroff)
                $ELEVATE_CMD \
                    zlogin -S "$ZONENAME" "poweroff"
                return $?
                ;;
            shutdown)
                $ELEVATE_CMD \
                    zlogin -S "$ZONENAME" "init 5" || \
                $ELEVATE_CMD \
                    zlogin -S "$ZONENAME" "shutdown" || \
                $ELEVATE_CMD \
                    zoneadm -z "$ZONENAME" "$ZONEACTION"
                return $?
                ;;
        esac
    fi
    logmsg_error "Unsupported action '$ZONEACTION' for zone '$ZONENAME'"
    return 1
}

boot_zone() {
    local ZONENAME="$1"
    zone_action boot "$ZONENAME"
}

zone_start_wait_svc() {
    local RES=0
    local ZONENAME="$1"
    local ZONEACT="$2"
    local WAITSVC_FMRI="$3"
    local WAITSVC_TEXT="$4"

    [ -n "$ZONENAME" ] || ZONENAME="${TARGET_ZONE}"
    [ -n "$ZONEACT" ] || ZONEACT="boot_zone"
    [ -n "$WAITSVC_FMRI" ] || WAITSVC_FMRI="svc:/milestone/multi-user-server:default"
    [ -n "$WAITSVC_TEXT" ] || WAITSVC_TEXT="${WAITSVC_FMRI}"

    # TBD: Review if any auto-vnics are defined, and if any of those are
    # missing on the host at this time; re-create them as needed.

    zone_action "$ZONEACT" "$ZONENAME" || \
        { RES=$? ; logmsg_error "Zone '$ZONENAME' did not '$ZONEACT'" ; return $RES; }

    get_zone_state | egrep -vi 'CONFIGURED|INCOMPLETE' >/dev/null || \
        { RES=$? ; logmsg_error "Zone '$ZONENAME' not running after trying to start it" ; return $RES; }

    # TODO: Possibility for infinite loop if something hangs inside the zone
    # See https://github.com/jimklimov/illumos-smf-zones for more of this logic
    logmsg_info "Waiting for $WAITSVC_TEXT to get reached in zone '$ZONENAME'..."
    sleep 5
    while : ; do
        $ELEVATE_CMD zlogin -S "$ZONENAME" \
                'svcadm enable -ts '"$WAITSVC_FMRI" \
            && return 0
        sleep 3
        get_zone_state "$ZONENAME" | egrep -i 'INSTALLED' >/dev/null && \
            { RES=$? ; logmsg_error "Zone '$ZONENAME' died after startup" ; return $RES; }
    done
    return $RES
}

singleuser_zone() {
    local ZONENAME="$1"
    zone_start_wait_svc "$ZONENAME" boot-single \
        svc:/milestone/single-user:default \
        "single-user milestone"
}

start_zone() {
    local ZONENAME="$1"
    zone_start_wait_svc "$ZONENAME" boot \
        svc:/milestone/multi-user-server:default \
        "multi-user-server milestone"
}

halt_zone() {
    local ZONENAME="$1"
    zone_action halt "$ZONENAME"
}

poweroff_zone() {
    local ZONENAME="$1"
    zone_action poweroff "$ZONENAME"
}

shutdown_zone() {
    local ZONENAME="$1"
    zone_action shutdown "$ZONENAME"
}

stop_zone() {
    local ZONENAME="$1"
    shutdown_zone "$ZONENAME" || \
    poweroff_zone "$ZONENAME"
    halt_zone "$ZONENAME"
}

restart_zone() {
    local ZONENAME="$1"
    stop_zone "$ZONENAME"
    start_zone "$ZONENAME"
}

reboot_zone() {
    local ZONENAME="$1"
    zone_action reboot "$ZONENAME"
}

# Note that operational states like "ready" and "mounted" are not visible
# from "zoneadm list" or /etc/zones/index - but they are visible in error
# messages when trying to change the zone state and request is invalid, e.g.:
#   zone 'test-giz-1': mount operation is invalid for zones in state 'ready'
#   zone 'test-giz-1': ready operation is invalid for zones in state 'mounted'
#   zone 'test-giz-1': mount operation is invalid for zones in state 'mounted'
#   zoneadm: zone 'test-giz-1': must be mounted before unmount.
#   zone 'test-giz-1': mount operation is invalid for zones in state 'running'
# Also note that of these, only "ready" is documented as a stable interface.
ready_zone() {
    local ZONENAME="$1"
    zone_action ready "$ZONENAME"
}

mount_zone() {
    local ZONENAME="$1"
    zone_action mount "$ZONENAME"
}

unmount_zone() {
    local ZONENAME="$1"
    zone_action unmount "$ZONENAME"
}

# Destroy a zone (including config, delegated datasets, zoneroot and VNICs)
destroy_zone() {
    local ZONENAME="$1"
    check_is_managed_zone "$ZONENAME" || return $?

    local ZONEROOT_MPT="`get_zone_root "${ZONENAME}"`" || return
    local ZONEROOT_DS="`zfs_getDSbyMPT "${ZONEROOT_MPT}"`" || return
    local DELEGATED_DATASETS="`list_assigned_datasets ${ZONENAME}`"

    delete_vnics_zone "$ZONENAME"

    stop_zone "$ZONENAME"
    unmount_zone "$ZONENAME"

    logmsg_info "Uninstalling zone '$ZONENAME'..."
    $ELEVATE_CMD zoneadm -z "$ZONENAME" uninstall -F
    logmsg_info "Removing configuration of zone '$ZONENAME'..."
    $ELEVATE_CMD zonecfg -z "$ZONENAME" delete -F

    local D
    for D in "$ZONEROOT_DS" $DELEGATED_DATASETS ; do
        if zfs_dataset_exists "$D" ; then
            # NOTE: For now leave this a message, not an automatic action
            # TBD: How does this influence clones etc.?
            # Should they be promoted first?
            logmsg_info "You may want to:  $ELEVATE_CMD zfs destroy -r $D"
        fi
    done
}

# Halt the zone if it was running, roll back to specified or most-recent
# snapshot (e.g. to reuse same blank zone content for another end-user
# job like a build), and restart if it was running. Note that technically
# the most-recent snapshot may be that of a most-recent GZ OS update, not
# of the stable state after last zone install/update as may be intended by
# sysadmin.
rollback_zone() {
    local ZONENAME="$1"
    local SNAPCOMMENT="$2"
    local D SNAP_RES

    logmsg_info "Rolling back zone '${ZONENAME}'@'${SNAPCOMMENT}'"
    check_is_managed_zone "$ZONENAME" || return

    if [ -z "$SNAPCOMMENT" ] ; then
        logmsg_info "No snapshot comment specified for the zone, using newest snapshot of zoneroot dataset"
        SNAPCOMMENT="$(get_zone_newest_snapshot_comment "$ZONENAME")" \
            && [ -n "$SNAPCOMMENT" ] || return
        logmsg_info "Rolling back zone '${ZONENAME}'@'${SNAPCOMMENT}'"
    fi

    local ZONEROOT_MPT="`get_zone_root "${ZONENAME}"`" || return
    local ZONEROOT_DS="`zfs_getDSbyMPT "${ZONEROOT_MPT}"`" || return

    local WAS_MOUNTED
    local WAS_RUNNING="`get_zone_state "$ZONENAME"`"

    # Stop zone? (option to force/forbid this?)
    if [ "$WAS_RUNNING" = running ]; then
        logmsg_info "Stopping zone ${ZONENAME} before rollback..."
        stop_zone "${ZONENAME}"
    fi

    if is_mounted "$ZONEROOT_MPT" ; then
        logmsg_info "Unmounting zone ${ZONENAME} before rollback..."
        unmount_zone "${ZONENAME}" # Can report error if zone not in mounted state
        # Delegated datasets are not visible in GZ; they should have been
        # handled by umount_zone(). Also below we umount non-zfs if used.
        # NOTE: Even with ELEVATE_CMD, a non-root fails to see into zonedir
        local ZONEROOT_MPT_ESCAPED="`echo "$ZONEROOT_MPT/" | sed 's,/,\\\\/,g'`"
        for D in \
            $(mount | awk '( $1 ~ /^'"$ZONEROOT_MPT_ESCAPED"'/ ) {print $1" "$3}' | sort -r) \
            "$ZONEROOT_MPT" \
        ; do
            logmsg_info "Trying to unmount $D ..."
            $ELEVATE_CMD umount "$D" || \
            $ELEVATE_CMD umount -f "$D" || \
            $ELEVATE_CMD zfs umount "$D" # || return
            # Note we do not necessarily abort here - rollbacks might
            # be able to proceed even while datasets are mounted.
            # If they do fail - we account for that below.
        done
        WAS_MOUNTED=yes
    else
        WAS_MOUNTED=no
    fi

    # Roll back zoneroot container dataset + delegated datasets. Per manpage:
#       The -rR options do not recursively destroy the child snapshots of a
#       recursive snapshot.  Only direct snapshots of the specified filesystem
#       are destroyed by either of these options. To completely roll back a
#       recursive snapshot, you must rollback the individual child snapshots.
    # By grepping for slash, we ensure not-manipulation of pool root datasets
    # due to some error, and avoid the "no datasets available" message (e.g.
    # we do not often expect volumes to be present, but cater for them here).

    # NOTE: This operation might fail when other datasets are cloned off
    # newer snapshots (e.g. reverting a GIZ from which DIZ's are already
    # instantiated). This can be amended by "zfs promote" so the lineage
    # of snapshots is reassigned to another head dataset and this one
    # becomes its (removable) clone, but such changes should be decided
    # and done by a human admin - so we do not automate them here now.
    SNAP_RES=0
    for D in "$ZONEROOT_DS" `list_assigned_datasets ${ZONENAME}` ; do
        for DD in $(zfs list -Honame -t filesystem -r "$D"|grep /) $(zfs list -Honame -t volume -r "$D"|grep /) ; do
            zfs list "$DD@$SNAPCOMMENT" > /dev/null 2>&1 || \
                { SNAP_RES=$?
                  logmsg_warn "Snapshot $DD@$SNAPCOMMENT did not exist, can not roll back"
                  continue; }
            logmsg_info "Rolling back to $DD@$SNAPCOMMENT ..."
	    $ELEVATE_CMD zfs rollback -r "$DD@$SNAPCOMMENT" || SNAP_RES=$?
        done
    done
    [ "$SNAP_RES" = 0 ] || \
        logmsg_warn "Could not roll back some datasets, seek details above." \
        "Maybe you need to zfs-promote datasets for a clone of this zone?"

    # Restart zone? (if was stopped by us)
    if [ "$WAS_MOUNTED" = yes ]; then
        logmsg_info "Trying to mount back datasets for ${ZONENAME}..."
        $ELEVATE_CMD zfs mount "$ZONEROOT_DS" || \
        $ELEVATE_CMD zfs mount "$ZONEROOT_MPT"
    fi

    # Check if zoneconfig now differs from one in the snapshot,
    # reapply old config if needed.
    if [ -s "$ZONEROOT_MPT/$ZONENAME.xml" ] ; then
        # TODO: Port the elevated-privilege exporter from snapshot_zone()
        # or just use mktemp directory
        $ELEVATE_CMD zoneadm -z "$ZONENAME" detach -n > "$ZONEROOT_MPT/$ZONENAME.xml.new"
        if ! diff "$ZONEROOT_MPT/$ZONENAME.xml" "$ZONEROOT_MPT/$ZONENAME.xml.new" >/dev/null ; then
            logmsg_info "Zone configuration of ${ZONENAME} differs from what we had in the past"
            TBD # Attach? Pass copies of whole zonecfg xml's or export's?
        fi
    fi

    if [ "$WAS_RUNNING" = running ]; then
        logmsg_info "Restarting zone ${ZONENAME} after rollback, because it was running before..."
        start_zone "${ZONENAME}"
    fi
    return $SNAP_RES
}

destroy_zone_snapshot() {
    local ZONENAME="$1"
    local SNAPCOMMENT="$2"
    local D SNAP_RES

    logmsg_info "Destroying snapshot '${SNAPCOMMENT}' of zone '${ZONENAME}'"
    check_is_managed_zone "$ZONENAME" || return

    TBD
    # TODO: a simplified variant of destroy_zone() and rollback_zone()
    # codebase, with purpose to consistently drop intermediate snapshots
    # of zone-related datasets that are deemed unneeded by an admin user.
    # Nothing to remount, restart, etc. - though should hiccup on clones
    # and similar cases that can not be destroyed silently.
}

# Run a command inside the zone (via zlogin), maybe as a specified account
run_in_zone() {
    if [ -z "${EXEC_CMD}" ]; then
        logmsg_error "run_in_zone(): nothing requested (EXEC_CMD is empty)"
        return 255
    fi
    check_is_managed_zone "$TARGET_ZONE" || return $?

    ZLOGIN_FLAGS=""
    case "${EXEC_USER}" in
        "") ;; # Use default account, e.g. local-zone root
        -S) ZLOGIN_FLAGS="-S" ;; # Recovery access bypassing logins
        *)  ZLOGIN_FLAGS="-l ${EXEC_USER}" ;;
    esac

    logmsg_info "Running '${EXEC_CMD}' as '${EXEC_USER}' in zone '${TARGET_ZONE}'"
    RUN_RES=0
    $ELEVATE_CMD \
        zlogin $ZLOGIN_FLAGS "${TARGET_ZONE}" "${EXEC_CMD}" || RUN_RES=$?

    if [ "$RUN_RES" = 0 ] ; then
        logmsg_info "SUCCESS: Running '${EXEC_CMD}' as '${EXEC_USER}' in zone '${TARGET_ZONE}'"
    else
        logmsg_warn "FAILED ($RUN_RES): Running '${EXEC_CMD}' as '${EXEC_USER}' in zone '${TARGET_ZONE}'"
    fi

    return $RUN_RES
}

##########################################################################
# Wrap the single-use zone routine
##########################################################################
diz_run() {
    # Create the DIZ, boot it, run the command, halt, destroy (e.g. one build)
    # Stop gracefully just in case something is saved to external storage?
    # TODO: Graceful shutdown vs fast halt based on autofs/delegated setup?

    # NOTE: This command does not require TARGET_ZONE to be pre-set, because
    # for single-use it might generate a random name and then discard it.
    DIZ_RUN_RES=255
    clone_zone && \
    start_zone "$TARGET_ZONE" && \
    { run_in_zone ; DIZ_RUN_RES=$?; }
    STACK_RES=$?
    if [ "$STACK_RES" = 0 ] && [ "$DIZ_RUN_RES" != 0 ] ; then
        STACK_RES="$DIZ_RUN_RES"
    fi

    logmsg_info "Exec ended with codes $STACK_RES for the stack and $DIZ_RUN_RES for the run"

    if [ "$STACK_RES" = 0 -a "$EXEC_DISCARD" = onsuccess ] || \
       [ "$EXEC_DISCARD" = always ] ; then
        logmsg_info "Discarding the zone '$TARGET_ZONE' because exec ended with code $STACK_RES and EXEC_DISCARD='$EXEC_DISCARD'"
        stop_zone "$TARGET_ZONE"
        destroy_zone
    else
        logmsg_info "Keeping the zone '$TARGET_ZONE' `get_zone_state $TARGET_ZONE 2>/dev/null` because exec ended with code $STACK_RES and EXEC_DISCARD='$EXEC_DISCARD'"
    fi

    return $DIZ_RUN_RES
}

do_zonemgr() {
    ##########################################################################
    # Do the run-time work
    ##########################################################################

    # Allow to togle shell-tracing in ZONEMGR-driven builds more easily
    if [ "$ZONEMGR_DEBUG" -ge "$ZONEMGR_DEBUGLEVEL_TRACEEXEC" ] ; then
        logmsg_info "ZONEMGR_DEBUG is $ZONEMGR_DEBUG >= $ZONEMGR_DEBUGLEVEL_TRACEEXEC : enabling source-code tracing!" >&2
        set -x
    fi

    settraps "exit_cleanup"

    parse_config_startup "$@"

    # CLI args or config files could change the setting
    if [ "$ZONEMGR_DEBUG" -ge "$ZONEMGR_DEBUGLEVEL_TRACEEXEC" ] ; then
        logmsg_info "ZONEMGR_DEBUG is $ZONEMGR_DEBUG >= $ZONEMGR_DEBUGLEVEL_TRACEEXEC : enabling source-code tracing!" >&2
        set -x
    fi

    # Make sure container-datasets are available
    logmsg_info "Ensuring availability of container dataset for GIZes"
    create_dataset_containment_GIZ
    DATASET_CONTAINMENT_GIZ_MPT="$(zfs_getMPTbyDS "$DATASET_CONTAINMENT_GIZ")" \
        && [ -d "$DATASET_CONTAINMENT_GIZ_MPT" ] \
        || die "Not a directory: DATASET_CONTAINMENT_GIZ_MPT='$DATASET_CONTAINMENT_GIZ_MPT'"
    $ELEVATE_CMD /bin/df -k "$DATASET_CONTAINMENT_GIZ_MPT" 2>/dev/null >/dev/null \
        || die "Not mounted: DATASET_CONTAINMENT_GIZ_MPT='$DATASET_CONTAINMENT_GIZ_MPT'"

    logmsg_info "Ensuring availability of container dataset for DIZes"
    create_dataset_containment_DIZ
    DATASET_CONTAINMENT_DIZ_MPT="$(zfs_getMPTbyDS "$DATASET_CONTAINMENT_DIZ")" \
        && [ -d "$DATASET_CONTAINMENT_DIZ_MPT" ] \
        || die "Not a directory: DATASET_CONTAINMENT_DIZ_MPT='$DATASET_CONTAINMENT_DIZ_MPT'"
    $ELEVATE_CMD /bin/df -k "$DATASET_CONTAINMENT_DIZ_MPT" 2>/dev/null >/dev/null \
        || die "Not mounted: DATASET_CONTAINMENT_DIZ_MPT='$DATASET_CONTAINMENT_DIZ_MPT'"

    # Remember to use check_is_managed_zone() before acting on existing zones
    case "$ACTION" in
        create-giz)
            install_zone ;;
        create-diz)
            # Would first create a matching GIZ if missing
            clone_zone ;;
        list)
            logmsg_info "Listing all managed zones..."
            list_zones_managed_all || clean_exit "No such zones were found" ;;
        list-giz)
            logmsg_info "Listing GIZ managed zones..."
            list_zones_managed_giz || clean_exit "No such zones were found" ;;
        list-diz)
            logmsg_info "Listing DIZ managed zones..."
            list_zones_managed_diz || clean_exit "No such zones were found" ;;
        is-managed-any)
            [ -n "$TARGET_ZONE" ] || die "No zone specified, while required for action '$ACTION'"
            if is_managed_zone "$TARGET_ZONE" ; then
                logmsg_info "Zone '$TARGET_ZONE' exists and is manageable by this script"
            else
                logmsg_warn "Zone '$TARGET_ZONE' does not exist or is not manageable by this script"
                clean_exit 1
            fi
            ;;
        is-managed-diz)
            [ -n "$TARGET_ZONE" ] || die "No zone specified, while required for action '$ACTION'"
            if is_managed_zone_diz "$TARGET_ZONE" ; then
                logmsg_info "Zone '$TARGET_ZONE' exists and is a DIZ manageable by this script"
            else
                logmsg_warn "Zone '$TARGET_ZONE' does not exist or is not manageable by this script as a DIZ"
                clean_exit 1
            fi
            ;;
        is-managed-giz)
            [ -n "$TARGET_ZONE" ] || die "No zone specified, while required for action '$ACTION'"
            if is_managed_zone_giz "$TARGET_ZONE" ; then
                logmsg_info "Zone '$TARGET_ZONE' exists and is a GIZ manageable by this script"
            else
                logmsg_warn "Zone '$TARGET_ZONE' does not exist or is not manageable by this script as a GIZ"
                clean_exit 1
            fi
            ;;
        verify)
            [ -n "$TARGET_ZONE" ] || die "No zone specified, while required for action '$ACTION'"
            if verify_zone "$TARGET_ZONE" ; then
                logmsg_info "Zone '$TARGET_ZONE' exists, has valid manifest and is manageable by this script"
            else
                logmsg_warn "Zone '$TARGET_ZONE' does not exist, has broken manifest, or is not manageable by this script"
                clean_exit 1
            fi
            ;;
        destroy)
            check_is_managed_zone "$TARGET_ZONE" || die
            if [ -n "$SNAP_NAME" ] ; then
                destroy_zone_snapshot "$TARGET_ZONE" "$SNAP_NAME"
            else
                destroy_zone "$TARGET_ZONE"
            fi
            ;;
        destroy-snapshot)
            check_is_managed_zone "$TARGET_ZONE" || die
            [ -n "$SNAP_NAME" ] || die "Snapshot name is required for destroy-snapshot"
            destroy_zone_snapshot "$TARGET_ZONE" "$SNAP_NAME"
            ;;
        snapshot)
            check_is_managed_zone "$TARGET_ZONE" || die
            snapshot_zone "$TARGET_ZONE" "$SNAP_NAME"
            ;;
        rollback)
            check_is_managed_zone "$TARGET_ZONE" || die
            rollback_zone "$TARGET_ZONE" "$SNAP_NAME"
            ;;
        list-snapshots)
            check_is_managed_zone "$TARGET_ZONE" || die
            list_zone_snapshot_datasets "$TARGET_ZONE" | pipe_get_snapshot_comment || die
            ;;
        pkg-refresh) TBD ;;
        pkg-upgrade) TBD ;;
        pkg-install) TBD ;;
        pkg-uninstall) TBD ;;
        exec)
            check_is_managed_zone "$TARGET_ZONE" || die
            secure_assigned_datasets "${TARGET_ZONE}"
            OLD_ZONE_STATE="`get_zone_state "$TARGET_ZONE"`"
            if [ "$OLD_ZONE_STATE" != running ] ; then
                start_zone "$TARGET_ZONE" || die
            fi
            RUN_RES=0
            run_in_zone || RUN_RES=$?
            if [ "$OLD_ZONE_STATE" != running ] ; then
                stop_zone "$TARGET_ZONE" || die
            fi
            clean_exit $RUN_RES
            ;;
        exec-diz)
            diz_run ;;
        get-state)
            get_zone_state "${TARGET_ZONE}" ;;
        boot|singleuser|halt|poweroff|shutdown|reboot|stop|start|restart|ready|mount|unmount)
            eval "$ACTION"_zone "$TARGET_ZONE" \
                || clean_exit "Failed to '$ACTION' the '$TARGET_ZONE' zone"
            ;;
        "") echo "ERROR: No (valid) required arguments were provided" >&2
            usage
            exit 1
            ;;
        *) echo "DEBUG: Would do ACTION='$ACTION' but this is not implemented (yet?)" >&2 ; exit 1 ;;
    esac

    clean_exit
}

### Support including this file into other tools or a scripted test suite
if [ -z "${ZONEMGR_IS_INCLUDED-}" ] ; then
    do_zonemgr "$@"
    exit
fi
true

